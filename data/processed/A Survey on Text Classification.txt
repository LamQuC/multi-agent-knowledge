111A Survey on Text Classification: From Traditional to Deep
Learning
QIAN LI, Beihang University, China
HAO PENG, Beihang University, China
JIANXIN LI‚àó,Beihang University, China
CONGYING XIA, University of Illinois at Chicago, USA
RENYU YANG, University of Leeds, UK
LICHAO SUN, Lehigh University, USA
PHILIP S. YU, University of Illinois at Chicago, USA
LIFANG HE, Lehigh University, USA
Text classification is the most fundamental and essential task in natural language processing. The last decade
has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods,
datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive
and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021,
focusing on models from traditional models to deep learning. We create a taxonomy for text classification
according to the text involved and the models used for feature extraction and classification. We then discuss
each of these categories in detail, dealing with both the technical developments and benchmark datasets that
support tests of predictions. A comprehensive comparison between different techniques, as well as identifying
the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by
summarizing key implications, future research directions, and the challenges facing the research area.
Additional Key Words and Phrases: deep learning, traditional models, text classification, evaluation metrics,
challenges.
ACM Reference Format:
Qian Li, Hao Peng, Jianxin Li, Congying Xia, Renyu Yang, Lichao Sun, Philip S. Yu, and Lifang He. 2021.
A Survey on Text Classification: From Traditional to Deep Learning. ACM Trans. Intell. Syst. Technol. 37, 4,
Article 111 (April 2021), 39 pages. https://doi.org/10.1145/1122445.1122456
1 INTRODUCTION
Text classification ‚Äì the procedure of designating pre-defined labels for text ‚Äì is an essential
and significant task in many Natural Language Processing (NLP) applications, such as sentiment
analysis [ 1,2], topic labeling [ 3,4], question answering [ 5,6] and dialog act classification [ 7]. In
‚àóCorresponding author
Authors‚Äô addresses: Qian Li, Beihang University, Haidian, Beijing, China, liqian@act.buaa.edu.cn; Hao Peng, Beihang
University, Haidian, Beijing, China, penghao@act.buaa.edu.cn; Jianxin Li, Beihang University, Haidian, Beijing, China,
lijx@act.buaa.edu.cn; Congying Xia, University of Illinois at Chicago, Chicago, IL, USA, cxia8@uic.edu; Renyu Yang,
University of Leeds, Leeds, England, UK, r.yang1@leeds.ac.uk; Lichao Sun, Lehigh University, Bethlehem, PA, USA, james.
lichao.sun@gmail.com; Philip S. Yu, University of Illinois at Chicago, Chicago, IL, USA, psyu@uic.edu; Lifang He, Lehigh
University, Bethlehem, PA, USA, lih319@lehigh.edu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
¬©2021 Association for Computing Machinery.
2157-6904/2021/4-ART111 $15.00
https://doi.org/10.1145/1122445.1122456
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.arXiv:2008.00364v6  [cs.CL]  22 Dec 2021111:2 Qian Li, et al.
Text
 Features Extraction
BoW/TF-IDFClassifier
NB/KNN/SVM/DT/RFTraditional Method
Evaluation
Accuracy/F1/Micro-F1
Deep Learning Method
Models
ReNN/MLP/RNN/CNN/Attention/Transformer/GCNLabel
Sentiment/TopicPreprocess
Fig. 1. Flowchart of the text classification with classic methods in each module. It is crucial to extract essential
features for traditional methods, but features can be extracted automatically by deep learning methods.
the era of information explosion, it is time-consuming and challenging to process and classify large
amounts of text data manually. Besides, the accuracy of manual text classification can be easily
influenced by human factors, such as fatigue and expertise. It is desirable to use machine learning
methods to automate the text classification procedure to yield more reliable and less subjective
results. Moreover, this can also help enhance information retrieval efficiency and alleviate the
problem of information overload by locating the required information.
Fig. 1 illustrates a flowchart of the procedures involved in the text classification, under the light
of traditional and deep analysis. Text data is different from numerical, image, or signal data. It
requires NLP techniques to be processed carefully. The first important step is to preprocess text data
for the model. Traditional models usually need to obtain good sample features by artificial methods
and then classify them with classic machine learning algorithms. Therefore, the effectiveness of the
method is largely restricted by feature extraction. However, different from traditional models, deep
learning integrates feature engineering into the model fitting process by learning a set of nonlinear
transformations that serve to map features directly to outputs.
From the 1960s until the 2010s, traditional text classification models dominated. Traditional
methods mean statistics-based models, such as Na√Øve Bayes (NB) [ 8], K-Nearest Neighbor (KNN)
[9], and Support Vector Machine (SVM) [ 10]. Comparing with the earlier rule-based methods, this
method has obvious advantages in accuracy and stability. However, these approaches still need to
do feature engineering, which is time-consuming and costly. Besides, they usually disregard the
natural sequential structure or contextual information in textual data, making it challenging to learn
the semantic information of the words. Since the 2010s, text classification has gradually changed
from traditional models to deep learning models. Compared with the methods based on traditional,
deep learning methods avoid designing rules and features by humans and automatically provide
semantically meaningful representations for text mining. Therefore, most of the text classification
research works are based on Deep Neural Networks (DNNs) [ 11], which are data-driven approaches
with high computational complexity. Few works focus on traditional models to settle the limitations
of computation and data.
1.1 Major Differences and Contributions
There have been several works reviewing text classification and its subproblems recently. Two of
them are reviews of text classification. Kowsari et al. [ 12] surveyed different text feature extraction,
dimensionality reduction methods, basic model structure for text classification, and evaluation
methods. Minaee et al. [ 13] reviewed recent deep learning based text classification methods,
benchmark datasets, and evaluation metrics. Unlike existing text classification reviews, we conclude
existing models from traditional models to deep learning with works of recent years. Traditional
models emphasize the feature extraction and classifier design. Once the text has well-designed
characteristics, it can be quickly converged by training the classifier. DNNs can perform feature
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:3
extraction automatically and learn well without domain knowledge. We then give the datasets and
evaluation metrics for single-label and multi-label tasks and summarize future research challenges
from data, models, and performance perspective. Moreover, we summarize various information
in three tables, including the necessary information of classic deep learning models, primary
information of main datasets, and a general benchmark of state-of-the-art methods under different
applications. In summary, this study‚Äôs main contributions are as follows:
‚Ä¢We introduce the process and development of text classification and present comprehensive
analysis and research on primary models ‚Äì from traditional to deep learning models ‚Äì
according to their model structures. We summarize the necessary information of deep learning
models in terms of basic model structures in Table 1, including publishing years, methods,
venues, applications, evaluation metrics, datasets and code links.
‚Ä¢We introduce the present datasets and give the formulation of main evaluation metrics with
the comparison of metrics, including single-label and multi-label text classification tasks. We
summarize the necessary information of primary datasets in Table 2, including the number
of categories, average sentence length, the size of each dataset, related papers and data
addresses.
‚Ä¢We summarize classification accuracy scores of models given in their articles, on benchmark
datasets in Table 4 and conclude the survey by discussing the main challenges facing the text
classification and key implications stemming from this study.
1.2 Organization of the Survey
The rest of the survey is organized as follows. Section 2 summarizes the existing models related
to text classification, including traditional and deep learning models, including a summary table.
Section 3 introduces the primary datasets with a summary table and evaluation metrics on single-
label and multi-label tasks. We then give quantitative results of the leading models in classic text
classification datasets in Section 4. Finally, we summarize the main challenges for deep learning
text classification in Section 5 before concluding the article in Section 6.
2 TEXT CLASSIFICATION METHODS
Text classification is referred to as extracting features from raw text data and predicting the
categories of text data based on such features. Numerous models have been proposed in the
past few decades for text classification. For traditional models, NB [ 8] is the first model used for
the text classification task. Whereafter, generic classification models are proposed, such as KNN
[9], SVM [ 10], and Random Forest (RF) [ 14], which are called classifiers, widely used for text
classification. Recently, the eXtreme Gradient Boosting (XGBoost) [ 15] and the Light Gradient
Boosting Machine (LightGBM) [ 16] have arguably the potential to provide excellent performance.
For deep learning models, TextCNN [ 17] has the highest number of references in these models,
wherein a Convolutional Neural Network (CNN) [ 18] model has been introduced to solve the
text classification problem for the first time. While not specifically designed for handling text
classification tasks, the Bidirectional Encoder Representation from Transformers (BERT) [ 19] has
been widely employed when designing text classification models, considering its effectiveness on
numerous text classification datasets.
2.1 Traditional Models
Traditional models accelerate text classification with improved accuracy and make the applica-
tion scope of traditional expand. The first thing is to preprocess the raw input text for training
traditional models, which generally consists of word segmentation, data cleaning, and statistics.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:4 Qian Li, et al.
deep
What is learning ?SUM
(a) CBOW.
deepWhat is learning ? (b) Skip-gram.
Fig. 2. The structure of word2vec, including CBOW and Skip-gram.
Then, text representation aims to express preprocessed text in a form that is much easier for
computers and minimizes information loss, such as Bag-Of-Words (BOW) [ 20], N-gram [ 21], Term
Frequency-Inverse Document Frequency (TF-IDF) [ 22], word2vec [ 23] and Global Vectors for word
representation (GloVe) [ 24]. BOW means that all the words in the corpus are formed into a mapping
array. According to the mapping array, a sentence can be represented as a vector. The ùëñ-th element
in the vector represents the frequency of the ùëñ-th word in the mapping array of the sentence.
The vector is the BOW of the sentence. At the core of the BOW is representing each text with a
dictionary-sized vector. The individual value of the vector denotes the word frequency correspond-
ing to its inherent position in the text. Compared to BOW, N-gram considers the information of
adjacent words and builds a dictionary by considering the adjacent words. It is used to calculate the
probability model of a sentence. The probability of a sentence is expressed as the joint probability
of each word in the sentence. The probability of a sentence can be calculated by predicting the
probability of the ùëÅ-th word, given the sequence of the (ùëÅ‚àí1)-th words. To simplify the calculation,
the N-gram model adopts the Markov hypothesis [ 21]. A word appears only concerning the words
that preceded it. Therefore, the N-gram model performs a sliding window with size N. By counting
and recording the occurrence frequency of all fragments, the probability of a sentence can be
calculated using the frequency of relevant fragments in the record. TF-IDF [ 22] uses the word
frequency and inverses the document frequency to model the text. TF is the word frequency of a
word in a specific article, and IDF is the reciprocal of the proportion of the articles containing this
word to the total number of articles in the corpus. TF-IDF is the multiplication of the two. TF-IDF
assesses the importance of a word to one document in a set of files or a corpus. The importance of
a word increases proportionally with the number of times it appears in a document. However, it
decreases inversely with its frequency in the corpus as a whole. The word2vec [ 23] employs local
context information to obtain word vectors, as shown in Fig. ??. Word vector refers to a fixed-length
real value vector specified as the word vector for any word in the corpus. The word2vec uses two
essential models: CBOW and Skip-gram. The former is to predict the current word on the premise
that the context of the current word is known. The latter is to predict the context when the current
word is known. The GloVe [ 24] ‚Äì with both the local context and global statistical features ‚Äì trains
on the nonzero elements in a word-word co-occurrence matrix, as shown in Fig. ??. It enables word
vectors to contain as much semantic and grammatical information as possible. The construction
method of word vector is: firstly, the co-occurrence matrix of words is constructed based on the
corpus, and then the word vector is learned based on the co-occurrence matrix and GloVe model.
Finally, the represented text is fed into the classifier according to selected features. Here, we discuss
some representative classifiers in detail:
2.1.1 PGM-based methods. Probabilistic Graphical Models (PGMs) express the conditional de-
pendencies among features in graphs, such as the Bayesian network [ 25]. It is combinations of
probability theory and graph theory.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:5
‚Ä¶1T2Ty )(Py
)|T(P1y )|T(P yn nT1-nT )|T(P1-yn 3T )|T(P2y )|T(P3y4T)|T(P4y
Fig. 3. The structure of Na√Øve Bayes.
Na√Øve Bayes (NB) [ 8] is the simplest and most broadly used model based on applying Bayes‚Äô
theorem. The NB algorithm has an independent assumption: when the target value has been given,
the conditions between text ùëá=[ùëá1,ùëá2,¬∑¬∑¬∑,ùëáùëõ]are independent (see Fig. 3). The NB algorithm
primarily uses the prior probability to calculate the posterior probability P(ùë¶|T1,ùëá2,¬∑¬∑¬∑,ùëáùëõ)=
ùëù(ùë¶)√éùëõ
ùëó=1ùëù(ùëáùëó|ùë¶)√én
ùëó=1ùëù(ùëáùëó). Due to its simple structure, NB is broadly used for text classification tasks. Although
the assumption that the features are independent is sometimes not actual, it substantially simplifies
the calculation process and performs better. To improve the performance on smaller categories,
Schneider [ 26] proposes a feature selection score method through calculating KL-divergence [ 27]
between the training set and corresponding categories for multinomial NB text classification. Dai et
al. [28] propose a transfer learning method named Naive Bayes Transfer Classification (NBTC) to
settle the different distribution between the training set and the target set. It uses the EM algorithm
[29] to obtain a locally optimal posterior hypothesis on the target set. NB classifier is also used for
fake news detection [ 30], and sentiment analysis [ 31], which can be seen as a text classification
task. Bernoulli NB, Gaussian NB and Multinomial NB are three popular approaches of NB text
classification [ 32]. Multinomial NB performs slightly better than Bernoulli NB on few labeled
dataset[ 33]. Bayesian NB classifier with Gaussian event model [ 32] has been proven to be superior
to NB with multinomial event model on 20 Newsgroups (20NG) [34] and WebKB [35] datasets.
8T7T
9T10T
14T
15T
1T
2T4T
3T5T
6T7T
8T9T10T
11T
12T13T14T
15TT
1T
2T4T
3T5T
6T7T
8T9T10T
11T
12T13T14T
15TT
3T6T1T
2T4T
5T11T
12TT
13TDataset Dataset
Input Space Input SpaceFeature Space
Hyperplane
Fig. 4. The structure of KNN where ùëò=4(left) and the structure of SVM (right). Each node represents a text
and nodes with different contours represent different categories.
2.1.2 KNN-based Methods. At the core of the K-Nearest Neighbors (KNN) algorithm [ 9] is to
classify an unlabeled sample by finding the category with most samples on the ùëònearest samples.
It is a simple classifier without building the model and can decrease complexity through the
fasting process of getting ùëònearest neighbors. Fig. 4 showcases the structure of KNN. We can find ùëò
training texts approaching a specific text to be classified through estimating the in-between distance.
Hence, the text can be divided into the most common categories found in ùëòtraining set texts. The
improvement of KNN algorithm mainly includes feature similarity [ 36],ùêævalue [ 37] and index
optimization [ 38]. However, due to the positive correlation between model time/space complexity
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:6 Qian Li, et al.
and the amount of data, the KNN algorithm takes an unusually long time on the large-scale datasets
[39]. To decrease the number of selected features, Soucy et al. [ 40] propose a KNN algorithm
without feature weighting. It manages to find relevant features, building the inter-dependencies of
words by using a feature selection. When the data is extremely unevenly distributed, KNN tends to
classify samples with more data. The Neighbor-Weighted K-Nearest Neighbor (NWKNN) [ 41] is
proposed to improve classification performance on the unbalanced corpora. It casts a significant
weight for neighbors in a small category and a small weight for neighbors in a broad class.
2.1.3 SVM-based Methods. Cortes and Vapnik [ 42] propose Support Vector Machine (SVM) to
tackle the binary classification of pattern recognition. Joachims [ 10], for the first time, uses the
SVM method for text classification representing each text as a vector. As illustrated in Fig. 4,
SVM-based approaches turn text classification tasks into multiple binary classification tasks. In this
context, SVM constructs an optimal hyperplane in the one-dimensional input space or feature space,
maximizing the distance between the hyperplane and the two categories of training sets, thereby
achieving the best generalization ability. The goal is to make the distance of the category boundary
along the direction perpendicular to the hyperplane is the largest. Equivalently, this will result in
the lowest error rate of classification. Constructing an optimal hyperplane can be transformed into
a quadratic programming problem to obtain a globally optimal solution. Choosing the appropriate
kernel function [ 43] and feature selection [ 44] are of the utmost importance to ensure SVM can deal
with nonlinear problems and become a robust nonlinear classifier. Furthermore, active learning [ 45]
and adaptive learning [ 46] method are used for text classification to reduce the labeling effort based
on the supervised learning algorithm SVM. To analyze what the SVM algorithms learn and what
tasks are suitable, Joachims [ 47] proposes a theoretical learning model combining the statistical
traits with the generalization performance of an SVM, analyzing the features and benefits using a
quantitative approach. Transductive Support Vector Machine (TSVM) [ 48] is proposed to lessen
misclassifications of the particular test collections with a general decision function considering a
specific test set. It uses prior knowledge to establish a more suitable structure and study faster.
Majority Voting/Averaging
‚Ä¶
Decision Tree-1 Decision Tree-2 Decision Tree-N
Class-1 Class-2 Class-N
Final ClassSingle Decision Tree
Final ClassTextText
Feature1
Feature2 Feature3
Feature4 Feature5Feature1
Feature2 Feature3
Feature4 Feature5Feature2
Feature1 Feature3
Feature4 Feature5Feature1
Feature3 Feature5
Feature2 Feature4
A B A B BB A A B A B BB A
AB ABBB A
A B ABBB A‚Ä¶
Fig. 5. An example of DT (left) and the structure of RF (right). The nodes with the dotted outline represent
the nodes of the decision route. It has five features to predict whether each text belongs to category A or B.
2.1.4 DT-based Methods. Decision Trees (DT) [ 49] is a supervised tree structure learning method
‚Äì reflective of the idea of divide-and-conquer ‚Äì and is constructed recursively. It learns disjunctive
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:7
expressions and has robustness for the text with noise. As shown in Fig. 5, decision trees can
be generally divided into two distinct stages: tree construction and tree pruning. It starts at the
root node and tests the data samples (composed of instance sets, which have several attributes),
and divides the dataset into diverse subsets according to different results. A subset of datasets
constitutes a child node, and every leaf node in the decision tree represents a category. Constructing
the decision tree is to determine the correlation between classes and attributes, further exploited to
predict the record categories of unknown forthcoming types. The classification rules generated
by the decision tree algorithm are straight-forward, and the pruning strategy [ 50] can also help
reduce the influence of noise. Its limitation, however, mainly derives from inefficiency in coping
with explosively increasing data size. More specifically, the Iterative Dichotomiser 3 (ID3) [ 51]
algorithm uses information gain as the attribute selection criterion in the selection of each node
‚Äì It is used to select the attribute of each branch node, and then select the attribute having the
maximum information gain value to become the discriminant attribute of the current node. Based
on ID3, C4.5 [ 52] learns to obtain a map from attributes to classes, which effectively classifies
entities unknown to new categories. DT based algorithms usually need to train for each dataset,
which is low efficiency [ 53]. Thus, Johnson et al. [ 54] propose a DT-based symbolic rule system.
The method represents each text as a vector calculated by the frequency of each word in the text,
and induces rules from the training data. The learning rules are used for classifying the other
data, being similar to the training data. Furthermore, to reduce the computational costs of DT
algorithms, Fast Decision-Tree (FDT) [ 55] uses a two-pronged strategy: pre-selecting a feature
set and training multiple DTs on different data subsets. Results from multiple DTs are combined
through a data-fusion technique to resolve the cases of imbalanced classes.
2.1.5 Integration-based Methods. Integrated algorithms aim to aggregate the results of multiple
algorithms for better performance and interpretation. Conventional integrated algorithms are
bootstrap aggregation, such as RF [ 14], boosting such as the Adaptive Boosting (AdaBoost) [ 56],
and XGBoost [ 15] and stacking. The bootstrap aggregation method trains multiple classifiers without
strong dependencies and then aggregates their results. For instance, RF [ 14] consists of multiple
tree classifiers wherein all trees depend on the value of the random vector sampled independently
(depicted in Fig. 5). It is worth noting that each tree within the RF shares the same distribution. The
generalization error of an RF relies on the strength of each tree and the relationship among trees,
and will converge to a limit with the increment of tree number in the forest. In boosting based
algorithms, all labeled data are trained with the same weight to initially obtain a weaker classifier
[57]. The weights of the data will then be adjusted according to the former result of the classifier.
The training procedure will continue by repeating such steps until the termination condition is
reached. Unlike bootstrap and boosting algorithms, stacking based algorithms break down the data
intoùëõparts and use ùëõclassifiers to calculate the input data in a cascade manner ‚Äì Result from
upstream classifier will feed into the downstream classifier as input. The training will terminate
once a pre-defined iteration number is targeted. The integrated method can capture more features
from multiple trees. However, it helps little for short text. Motivated by this, Bouaziz et al. [ 58]
combine data enrichment ‚Äì with semantics in RFs for short text classification ‚Äì to overcome the
deficiency of sparseness and insufficiency of contextual information. In integrated algorithms, not
all classifiers learn well. It is necessary to give different weights for each classifier. To differentiate
contributions of trees in a forest, Islam et al. [ 59] exploit the Semantics Aware Random Forest
(SARF) classifier, choosing features similar to the features of the same class, for extracting features
and producing the prediction values.
Summary. The parameters of NB are more diminutive, less sensitive to missing data, and the
algorithm is simple. However, it assumes that features are independent of each other. When the
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:8 Qian Li, et al.
number of features is large, or the correlation between features is significant, the performance of
NB decreases. SVM can solve high-dimensional and nonlinear problems. It has a high generalization
ability, but it is sensitive to missing data. KNN mainly depends on the surrounding finite adjacent
samples, rather than discriminating class domain to determine the category. Thus, for the dataset
to be divided with more crossover or overlap of the class domain, it is more suitable than other
methods. DT is easy to understand and interpret. Given an observed model, it is easy to deduce the
corresponding logical expression according to the generated decision tree. The traditional method is
a type of machine learning. It learns from data, which are pre-defined features that are important to
the performance of prediction values. However, feature engineering is tough work. Before training
the classifier, we need to collect knowledge or experience to extract features from the original
text. The traditional methods train the initial classifier based on various textual features extracted
from the raw text. Toward small datasets, traditional models usually present better performance
than deep learning models under the limitation of computational complexity. Therefore, some
researchers have studied the design of traditional models for specific domains with fewer data.
2.2 Deep Learning Models
The DNNs consist of artificial neural networks that simulate the human brain to automatically learn
high-level features from data, getting better results than traditional models in speech recognition,
image processing, and text understanding. Input datasets should be analyzed to classify the data,
such as a single-label, multi-label, unsupervised, unbalanced dataset. According to the trait of the
dataset, the input word vectors are sent into the DNN for training until the termination condition
is reached. The performance of the training model is verified by the downstream task, such as
sentiment classification, question answering, and event prediction. We show some DNNs over
the years in Table 1, including designs that are different from the corresponding basic models,
evaluation metrics, and experimental datasets.
Numerous deep learning models have been proposed in the past few decades for text classification,
as shown in Table 1. We tabulate primary information ‚Äì including publication years, venues,
applications, code links, evaluation metrics, and experiment datasets ‚Äì of main deep learning
models for text classification. The applications in this table include Sentiment Analysis (SA), Topic
Labeling (TL), News Classification (NC), Question Answering (QA), Dialog Act Classification (DAC),
Natural Language Inference (NLI) and Relation Classification (RC). The multilayer perceptron [ 172]
and the recursive neural network [ 173] are the first two deep learning approaches used for the text
classification task, which improve performance compared with traditional models. Then, CNNs,
Recurrent Neural Networks (RNNs), and attention mechanisms are used for text classification
[101,174,175]. Many researchers advance text classification performance for different tasks by
improving CNN, RNN, and attention, or model fusion and multi-task methods. The appearance
of BERT [ 19], which can generate contextualized word vectors, is a significant turning point in
the development of text classification and other NLP technologies. Many researchers [ 142,176]
have studied text classification models based on BERT, which achieves better performance than
the above models in multiple NLP tasks, including text classification. Besides, some researchers
study text classification technology based on Graph Neural Network (GNN) [ 155,177] to capture
structural information in the text, which cannot be replaced by other methods. Here, we classify
DNNs by structure and discuss some representative models in detail:
2.2.1 ReNN-based Methods. Traditional models cost lots of time on design features for each
task. Furthermore, in the case of deep learning, the meaning of "word vectors" is different: each
input word is associated with a fixed-length vector whose values are either drawn at random or
derived from a previous traditional process, thus forming a matrix ùêøcalled word embedding matrix
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:9
Table 1. Basic information based on different models. Trans: Transformer. Time: training time.
Model Year Method Venue Applications Code Link Metrics Datasets
2011 RAE [60] EMNLP SA, QA [61] Accuracy MPQA, MR, EP
ReNN 2012 MV-RNN [62] EMNLP SA [63] Accuracy, F1 MR
2013 RNTN [64] EMNLP SA [65] Accuracy SST
2014 DeepRNN [66] NIPS SA;QA - Accuracy SST-1;SST-2
MLP 2014 Paragraph-Vec [67] ICML SA, QA [68] Error Rate SST, IMDB
2015 DAN [69] ACL SA, QA [70] Accuracy, Time RT, SST, IMDB
2015 Tree-LSTM [1] ACL SA [71] Accuracy SST-1, SST-2
2015 S-LSTM [2] ICML SA - Accuracy SST
2015 TextRCNN [72] AAAI SA, TL [73] Macro-F1, etc. 20NG, Fudan, ACL, SST-2
2015 MT-LSTM [6] EMNLP SA,QA [74] Accuracy SST-1, SST-2, QC, IMDB
2016 oh-2LSTMp [75] ICML SA, TL [76] Error Rate IMDB, Elec, RCV1, 20NG
RNN 2016 BLSTM-2DCNN [77] COLING SA, QA, TL [78] Accuracy SST-1, Subj, TREC, etc.
2016 Multi-Task [79] IJCAI SA [80] Accuracy SST-1, SST-2, Subj, IMDB
2017 DeepMoji [81] EMNLP SA [82] Accuracy SS-Twitter, SE1604, etc.
2017 TopicRNN [83] ICML SA [84] Error Rate IMDB
2017 Miyato et al. [85] ICLR SA [86] Error Rate IMDB, DBpedia, etc.
2018 RNN-Capsule [87] TheWebConf SA [88] Accuracy MR, SST-1, etc.
2019 HM-DenseRNNs [89] IJCAI SA, TL [90] Accuracy IMDB, SST-5, AG
2014 TextCNN [17] EMNLP SA, QA [91] Accuracy MR, SST-2, Subj, etc.
2014 DCNN [5] ACL SA, QA [92] Accuracy MR, TREC, Twitter
2015 CharCNN [93] NeurIPS SA, QA, TL [94] Error Rate AG, Yelp P, DBPedia, etc.
2016 SeqTextRCNN [7] NAACL Dialog act [95] Accuracy DSTC 4, MRDA, SwDA
2017 XML-CNN [96] SIGIR NC, TL, SA [97] NDCG@K, etc. EUR-Lex, Wiki-30K, etc.
CNN 2017 DPCNN [98] ACL SA, TL [99] Error Rate AG, DBPedia, Yelp.P, etc.
2017 KPCNN [100] IJCAI SA, QA, TL - Accuracy Twitter, AG, Bing, etc.
2018 TextCapsule [101] EMNLP SA, QA, TL [102] Accuracy Subj, TREC, Reuters, etc.
2018 HFT-CNN [103] EMNLP TL [104] Micro-F1, etc. RCV1, Amazon670K
2019 CCRCNN [105] AAAI TL - Accuracy TREC, MR, AG
2020 Bao et al. [106] ICLR TL [107] Accuracy 20NG, Reuters-2157, etc.
2016 HAN [108] NAACL SA, TL [109] Accuracy Yelp.F, YahooA, etc.
2016 BI-Attention [110] NAACL SA - Accuracy NLP&CC 2013 [111]
2016 LSTMN [112] EMNLP SA [113] Accuracy SST-1
2017 Lin et al. [114] ICLR SA [115] Accuracy Yelp, SNLI Age
2018 SGM [116] COLING TL [117] HL, Micro-F1 RCV1-V2, AAPD
2018 ELMo [118] NAACL SA, QA, NLI [119] Accuracy SQuAD, SNLI, SST-5
Attention 2018 BiBloSA [120] ICLR SA [121] Accuracy, Time CR, MPQA, SUBJ, etc.
2019 AttentionXML [122] NeurIPS TL [123] P@k, N@k, etc. EUR-Lex, etc.
2019 HAPN [124] EMNLP RC - Accuracy FewRel, CSID
2019 Proto-HATT [125] AAAI RC [126] Accuracy FewRel
2019 STCKA [127] AAAI SA, TL [128] Accuracy Weibo, Product Review, etc.
2020 HyperGAT [129] EMNLP TL, NC [130] Accuracy 20NG, Ohsumed, MR, etc.
2020 MSMSA [131] AAAI ST, QA, NLI - Accuracy, F1 IMDB, MR, SST, SNLI, etc.
2020 Choi [132] EMNLP SA, TL - Accuracy SST2, IMDB, 20NG
2019 BERT [19] NAACL SA, QA [133] Accuracy SST-2, QQP, QNLI, CoLA
2019 BERT-BASE [134] ACL TL [135] P@K, R@K, etc. EUR-LEX
2019 Sun et al. [136] CCL SA, QA, TL [137] Error Rate TREC, DBPedia, etc.
2019 XLNet [138] NeurIPS SA, QA, NC [139] EM, F1, etc. Yelp-2, AG, MNLI, etc.
2019 RoBERTa [140] arXiv SA, QA [141] F1, Accuracy SQuAD, MNLI-m, SST-2
Trans 2020 GAN-BERT [142] ACL SA, NLI [143] F1, Accuracy SST-5, MNLI
2020 BAE [144] EMNLP SA, QA [145] Accuracy Amazon, Yelp, MR, MPQA
2020 ALBERT [146] ICLR SA, QA [147] F1, Accuracy SST, MNLI, SQuAD
2020 TG-Transformer [148] EMNLP SA, TL - Accuracy, Time R8, R52, Ohsumed, etc.
2020 X-Transformer [149] KDD SA, TL [150] P@K, R@K Eurlex-4K, Wiki10-31K, etc.
2021 LightXML [151] arXiv TL, ML, NLI [152] P@K, Time AmazonCat-13K, etc.
2018 DGCNN [153] TheWebConf TL [154] Macro-F1, etc. RCV1, NYTimes
2019 TextGCN [155] AAAI SA, TL [156] Accuracy 20NG, Ohsumed, R52, etc.
2019 SGC[157] ICML NC, TL, SA [158] Accuracy, Time 20NG, R8, Ohsumed, etc.
GNN 2019 Huang et al. [159] EMNLP NC, TL [160] Accuracy R8, R52, Ohsumed
2019 Peng et al. [161] arXiv NC, TL - Micro-F1, etc. RCV1, EUR-Lex, etc.
2020 TextING [162] ACL SA, NC, TL [163] Accuracy MR, R8, R52, Ohsumed
2020 TensorGCN [164] AAAI SA, NC, TL [165] Accuracy 20NG, R8, R52, Ohsumed, MR
2020 MAGNET [166] ICAART TL [167] Micro-F1, HL Reuters, RCV1-V2, etc.
2017 Miyato et al. [85] ICLR SA, NC [168] Error Rate IMDB, RCV1, et al.
Others 2018 TMN [169] EMNLP TL - Accuracy, F1 Snippets, Twitter, et al.
2019 Zhang et al. [170] NAACL TL, NC [171] Accuracy DBpedia, 20NG.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:10 Qian Li, et al.
Label
b
What is deep learningActivation function
?Label
Branch
Branch
What is deep learning ?BranchBranch
0w1w2w3w 4w
Fig. 6. The architecture of ReNN (left) and the architecture of MLP (right).
which represents the vocabulary words in a small latent semantic space, of generally 50 to 300
dimensions. The Recursive Neural Network (ReNN) [ 173] can automatically learn the semantics of
text recursively and the syntax tree structure without feature design, as shown in Fig. 6. We give
an example of ReNN based models. First, each word of input text is taken as the leaf node of the
model structure. Then all nodes are combined into parent nodes using a weight matrix. The weight
matrix is shared across the whole model. Each parent node has the same dimension with all leaf
nodes. Finally, all nodes are recursively aggregated into a parent node to represent the input text to
predict the label.
ReNN-based models improve performance compared with traditional models and save on labor
costs due to excluding feature designs used for different text classification tasks. The Recursive
AutoEncoder (RAE) [ 60] is used to predict the distribution of sentiment labels for each input
sentence and learn the representations of multi-word phrases. To learn compositional vector
representations for each input text, the Matrix-Vector Recursive Neural Network (MV-RNN) [ 62]
introduces a ReNN model to learn the representation of phrases and sentences. It allows that the
length and type of input texts are inconsistent. MV-RNN allocates a matrix and a vector for each
node on the constructed parse tree. Furthermore, the Recursive Neural Tensor Network (RNTN)
[64] is proposed with a tree structure to capture the semantics of sentences. It inputs phrases with
different length and represents the phrases by parse trees and word vectors. The vectors of higher
nodes on the parse tree are estimated by the equal tensor-based composition function. For RNTN,
the time complexity of building the textual tree is high, and expressing the relationship between
documents is complicated within a tree structure. The performance is usually improved, with the
depth being increased for DNNs. Therefore, Irsoy et al. [ 66] propose a Deep Recursive Neural
Network (DeepReNN), which stacks multiple recursive layers. It is built by binary parse trees and
learns distinct perspectives of compositionality in language.
2.2.2 MLP-based Methods. A MultiLayer Perceptron (MLP) [ 172], sometimes colloquially called
"vanilla" neural network, is a simple neural network structure that is used for capturing features
automatically. As shown in Fig. 6, we show a three-layer MLP model. It contains an input layer, a
hidden layer with an activation function in all nodes, and an output layer. Each node connects with
a certain weight ùë§ùëñ. It treats each input text as a bag of words and achieves high performance on
many text classification benchmarks comparing with traditional models.
There are some MLP-based methods proposed by some research groups for text classification tasks.
The Paragraph Vector (Paragraph-Vec) [ 67] is the most popular and widely used method, which is
similar to the Continuous Bag-Of-Words (CBOW) [ 23]. It gets fixed-length feature representations
of texts with various input lengths by employing unsupervised algorithms. Comparing with CBOW,
it adds a paragraph token mapped to the paragraph vector by a matrix. The model predicts the
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:11
fourth word by the connection or average of this vector to the three contexts of the word. Paragraph
vectors can be used as a memory for paragraph themes and are used as a paragraph function and
inserted into the prediction classifier.
Label
ConvPoolingLabel
‚Ä¶
Concat
What is deeplearning ?11h12h31h41h51h12h22h23h24h25h
What is deeplearning ?
Fig. 7. The RNN based model (left) and the CNN based model (right).
2.2.3 RNN-based Methods. The Recurrent Neural Network (RNN) [ 173] is broadly used for cap-
turing long-range dependency through recurrent computation. The RNN language model learns
historical information, considering the location information among all words suitable for text
classification tasks. We show an RNN model for text classification with a simple sample, as shown
in Fig. 7. Firstly, each input word is represented by a specific vector using a word embedding
technology. Then, the embedding word vectors are fed into RNN cells one by one. The output of
RNN cells are the same dimension with the input vector and are fed into the next hidden layer. The
RNN shares parameters across different parts of the model and has the same weights of each input
word. Finally, the label of input text can be predicted by the last output of the hidden layer.
To diminish the time complexity of the model and capture contextual information, Liu et al. [ 79]
introduce a model for catching the semantics of long texts. It is a biased model that parsed the
text one by one, making the following inputs profit over the former and decreasing the semantic
efficiency of capturing the whole text. For modeling topic labeling tasks with long input sequences,
TopicRNN [ 83] is proposed. It captures the dependencies of words in a document via latent topics
and uses RNNs to capture local dependencies and latent topic models for capturing global semantic
dependencies. Virtual Adversarial Training (VAT) [ 178] is a useful regularization method applicable
to semi-supervised learning tasks. Miyato et al. [ 85] apply adversarial and virtual adversarial
training text and employ the perturbation into word embedding rather than the original input text.
The model improves the quality of the word embedding and is not easy to overfit during training.
Capsule network [ 179] captures the relationships between features using dynamic routing between
capsules comprised of a group of neurons in a layer. Wang et al. [ 87] propose an RNN-Capsule
model with a simple capsule structure for the sentiment classification task.
In the backpropagation process of RNN, the weights are adjusted by gradients, calculated by
continuous multiplications of derivatives. If the derivatives are extremely small, it may cause a
gradient vanishing problem by continuous multiplications. Long Short-Term Memory (LSTM) [ 180],
the improvement of RNN, effectively alleviates the gradient vanishing problem. It is composed of a
cell to remember values on arbitrary time intervals and three gate structures to control information
flow. The gate structures include input gates, forget gates, and output gates. The LSTM classification
method can better capture the connection among context feature words, and use the forgotten gate
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:12 Qian Li, et al.
structure to filter useless information, which is conducive to improving the total capturing ability
of the classifier. Tree-LSTM [ 1] extends the sequence of LSTM models to the tree structure. The
whole subtree with little influence on the result can be forgotten through the LSTM forgetting gate
mechanism for the Tree-LSTM model.
Natural Language Inference (NLI) [ 181] predicts whether one text‚Äôs meaning can be deduced
from another by measuring the semantic similarity between each pair of sentences. To consider
other granular matchings and matchings in the reverse direction, Wang et al. [ 182] propose a model
for the NLI task named Bilateral Multi-Perspective Matching (BiMPM). It encodes input sentences
by the BiLSTM encoder. Then, the encoded sentences are matched in two directions. The results
are aggregated in a fixed-length matching vector by another BiLSTM layer. Finally, the result is
evaluated by a fully connected layer.
2.2.4 CNN-based Methods. Convolutional Neural Networks (CNNs) [ 18] are proposed for image
classification with convolving filters that can extract features of pictures. Unlike RNN, CNN can
simultaneously apply convolutions defined by different kernels to multiple chunks of a sequence.
Therefore, CNNs are used for many NLP tasks, including text classification. For text classification,
the text requires being represented as a vector similar to the image representation, and text features
can be filtered from multiple angles, as shown in Fig. 7. Firstly, the word vectors of the input text
are spliced into a matrix. The matrix is then fed into the convolutional layer, which contains several
filters with different dimensions. Finally, the result of the convolutional layer goes through the
pooling layer and concatenates the pooling result to obtain the final vector representation of the
text. The category is predicted by the final vector.
To try using CNN for the text classification task, an unbiased model of convolutional neural
networks is introduced by Kim, called TextCNN [ 17]. It can better determine discriminative phrases
in the max-pooling layer with one layer of convolution and learn hyperparameters except for
word vectors by keeping word vectors static. Training only on labeled data is not enough for
data-driven deep models. Therefore, some researchers consider utilizing unlabeled data. Johnson et
al. [183] propose a CNN model based on two-view semi-supervised learning for text classification,
which first uses unlabeled data to train the embedding of text regions and then labeled data. DNNs
usually have better performance, but it increases the computational complexity. Motivated by
this, a Deep Pyramid Convolutional Neural Network (DPCNN) [ 98] is proposed, with a little more
computational accuracy, increasing by raising the network depth. The DPCNN is more specific
than Residual Network (ResNet) [ 184], as all the shortcuts are exactly simple identity mappings
without any complication for dimension matching.
According to the minimum embedding unit of text, embedding methods are divided into character-
level, word-level, and sentence-level embedding. Character-level embeddings can settle Out-Of-
Vocabulary (OOV) [ 185] words. Word-level embeddings learn the syntax and semantics of the
words. Moreover, sentence-level embedding can capture relationships among sentences. Motivated
by these, Nguyen et al. [ 186] propose a deep learning method based on a dictionary, increasing
information for word-level embeddings through constructing semantic rules and deep CNN for
character-level embeddings. Adams et al. [ 187] propose a character-level CNN model, called MGTC,
to classify multi-lingual texts written. TransCap [ 188] is proposed to encapsulate the sentence-level
semantic representations into semantic capsules and transfer document-level knowledge.
RNN based models capture the sequential information to learn the dependency among input
words, and CNN based models extract the relevant features from the convolution kernels. Thus some
works study the fusion of the two methods. BLSTM-2DCNN [ 77] integrates a Bidirectional LSTM
(BiLSTM) with two-dimensional max pooling. It uses a 2D convolution to sample more meaningful
information of the matrix and understands the context better through BiLSTM. Moreover, Xue
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:13
et al. [ 189] propose MTNA, a combination of BiLSTM and CNN layers, to solve aspect category
classification and aspect term extraction tasks.
21w22wTw221h 22h Th221h 22h Th2wU1s2sLs1h 2h Lh2h LhText vector (v)Activation function
1hsU1a
Word encoderSentence encoder
Word attentionSentence attention
21a2aLa
22a Ta2‚Ä¶‚Ä¶‚Ä¶
‚Ä¶‚Ä¶‚Ä¶‚Ä¶ ‚Ä¶: sentence context vector sU
: word context vector wU
h
h
21w: The first word in sentence 2: backward hidden state: forward hidden state1s: The first sentence in the textLabel
Fig. 8. The architecture of hierarchical attention network (HAN) [108].
2.2.5 Attention-based Methods. CNN and RNN provide excellent results on tasks related to text
classification. However, these models are not intuitive enough for poor interpretability, especially
in classification errors, which cannot be explained due to the non-readability of hidden data. The
attention-based methods are successfully used in the text classification. Bahdanau et al. [ 190] first
propose an attention mechanism that can be used in machine translation. Motivated by this, Yang
et al. [ 108] introduce the Hierarchical Attention Network (HAN) to gain better visualization by
employing the extremely informational components of a text, as shown in Fig. 8. HAN includes two
encoders and two levels of attention layers. The attention mechanism lets the model pay different
attention to specific inputs. It aggregates essential words into sentence vectors firstly and then
aggregates vital sentence vectors into text vectors. It can learn how much contribution of each
word and sentence for the classification judgment, which is beneficial for applications and analysis
through the two levels of attention.
The attention mechanism can improve the performance with interpretability for text classification,
which makes it popular. There are some other works based on attention. LSTMN [ 112] is proposed
to process text step by step from left to right and does superficial reasoning through memory
and attention. BI-Attention [ 110] is designed for cross-lingual text classification to catch bilingual
long-distance dependencies. Hu et al. [ 191] propose an attention mechanism based on category
attributes for solving the imbalance of the number of various charges which contain few-shot
charges. HAPN [124] is presented for few-shot text classification.
Self-attention [ 192] captures the weight distribution of words in sentences by constructing K, Q
and V matrices among sentences that can capture long-range dependencies on text classification. We
give an example for self-attention, as shown in Fig. 9. Each input word vector ùëéùëñcan be represented
as three n-dimensional vectors, including ùëûùëñ,ùëòùëñandùë£ùëñ. After self-attention, the output vector ùëèùëñ
can be represented as√ç
ùëóùë†ùëúùëìùë°ùëöùëéùë•(ùëéùëñ ùëó)ùë£ùëóandùëéùëñ ùëó=ùëûùëñ¬∑ùëòùëó/‚àöùëõ. All output vectors can be parallelly
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:14 Qian Li, et al.
computed. Lin et al. [ 114] used source token self-attention to explore the weight of every token
to the entire sentence in the sentence representation task. To capture long-range dependencies,
Bi-directional Block Self-Attention Network (Bi-BloSAN) [ 120] uses an intra-block Self-Attention
Network (SAN) to every block split by sequence and an inter-block SAN to the outputs.
What is deep learning ?21a22a 23a24a25a
2a1a 3a4a 5a2q2k2v 3q3k3v 1q1k1v4q4k4v 5q5k5v2b
Fig. 9. An example of self-attention for calculating output vector ùëè2.
Aspect-Based Sentiment Analysis (ABSA) [ 31,193] breaks down a text into multiple aspects
and allocates each aspect a sentiment polarity. The sentiment polarity can be divided into three
types: positive, neutral and negative. Some attention-based models are proposed to identify the
fine-grained opinion polarity towards a specific aspect for aspect-based sentiment tasks. ATAE-
LSTM [ 194] can concentrate on different parts of each sentence according to the input through the
attention mechanisms. MGAN [ 195] presents a fine-grained attention mechanism with a coarse-
grained attention mechanism to learn the word-level interaction between context and aspect.
To catch the complicated semantic relationship among each question and candidate answers for
the QA task, Tan et al. [ 196] introduce CNN and RNN and generate answer embeddings by using a
simple one-way attention mechanism affected through the question context. The attention captures
the dependence among the embeddings of questions and answers. Extractive QA can be seen as the
text classification task. It inputs a question and multiple candidates answers and classifies every
candidate answer to recognize the correct answer. Furthermore, AP-BILSTM [ 197] with a two-way
attention mechanism can learn the weights between the question and each candidate answer to
obtain the importance of each candidate answer to the question.
1E2ETrm Trm TrmTrm Trm Trm
‚Ä¶‚Ä¶‚Ä¶
NE2T‚Ä¶
1TNTBERT
1E2ETrm Trm TrmTrm Trm Trm
‚Ä¶‚Ä¶‚Ä¶
NE2T‚Ä¶
1TNTOpenAI GPT
1E2ELSTM LSTM LSTMLSTM LSTM LSTM
‚Ä¶‚Ä¶‚Ä¶
NE2T‚Ä¶
1TNTELMo
LSTM LSTM LSTMLSTM LSTM LSTM
‚Ä¶‚Ä¶
Fig. 10. Differences in pre-trained model architectures [ 19], including BERT, OpenAI GPT and ELMo. ùê∏ùëñ
represents embedding of ùëñth input. Trm represents the transformer block. ùëáùëñrepresents predicted tag of ùëñth
input.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:15
2.2.6 Pre-trained Methods. Pre-trained language models [ 198] effectively learn global semantic
representation and significantly boost NLP tasks, including text classification. It generally uses
unsupervised methods to mine semantic knowledge automatically and then construct pre-training
targets so that machines can learn to understand semantics.
As shown in Fig. 10, we give differences in the model architectures among the Embedding
from Language Model (ELMo) [ 118], OpenAI GPT [ 199], and BERT [ 19]. ELMo [ 118] is a deep
contextualized word representation model, which is readily integrated into models. It can model
complicated characteristics of words and learn different representations for various linguistic
contexts. It learns each word embedding according to the context words with the bi-directional
LSTM. GPT [ 199] employs supervised fine-tuning and unsupervised pre-training to learn general
representations that transfer with limited adaptation to many NLP tasks. Furthermore, the domain
of the target dataset does not need to be similar to the domain of unlabeled datasets. The training
procedure of the GPT algorithm usually includes two stages. Firstly, the initial parameters of
a neural network model are learned by a modeling objective on the unlabeled dataset. We can
then employ the corresponding supervised objective to accommodate these parameters for the
target task. To pre-train deep bidirectional representations from the unlabeled text through joint
conditioning on both left and right context in every layer, BERT model [ 19], proposed by Google,
significantly improves performance on NLP tasks, including text classification. BERT applies the
bi-directional encoder designed to pre-train the bi-directional representation of depth by jointly
adjusting the context in all layers. It can utilize contextual information when predicting which
words are masked. It is fine-tuned by adding just an additional output layer to construct models
for multiple NLP tasks, such as SA, QA, and machine translation. Comparing with these three
models, ELMo is a feature-based method using LSTM, and BERT and OpenAI GPT are fine-tuning
approaches using Transformer. Furthermore, ELMo and BERT are bidirectional training models and
OpenAI GPT is training from left to right. Therefore, BERT gets a better result, which combines
the advantages of ELMo and OpenAI GPT.
Transformer-based models can parallelize computation without considering the sequential
information suitable for large scale datasets, making it popular for NLP tasks. Thus, some other
works are used for text classification tasks and get excellent performance. RoBERTa [ 140], is an
improved version of BERT, adopts the dynamic masking method that generates the masking pattern
every time with a sequence to be fed into the model. It uses more data for longer pre-training and
estimates the influence of various essential hyperparameters and the size of training data. To be
specific: 1) The training time is longer (a total of nearly 200,000 training, nearly 1.6 billion training
data have been seen), the batch size (8K) is larger, and the training data is more (30G Chinese
training, including 300 million sentences and 10 billion words); 2) It removes the next sentence
prediction (NSP) task; 3) It employs more extended training sequence; 4) It dynamically adjusts the
masking mechanism and use the full word mask.
XLNet [ 138] is a generalized autoregressive pre-training approach. Unlike BERT, the denoising
autoencoder with the mask is not used in the first stage, but the autoregressive LM is used. It
maximizes the expected likelihood across the whole factorization order permutations to learn the
bidirectional context. Furthermore, it can overcome the weaknesses of BERT by an autoregressive
formulation and integrate ideas from Transformer-XL [200] into pre-training.
BERT model has many parameters. In order to reduce the parameters, ALBERT [ 146] uses
two-parameter simplification schemes. It reduces the fragmentation vector‚Äôs length and shares
parameters with all encoders. It also replaces the next sentence matching task with the next
sentence order task and continuously blocks fragmentation. When the ALBERT model is pre-
trained on a massive Chinese corpus, the parameters are less and better performance. In general,
these methods adopt unsupervised objective functions for pre-training, including the next sentence
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:16 Qian Li, et al.
prediction, masking technology, and permutation. These target functions based on the word
prediction demonstrate a strong ability to learn the word dependence and semantic structure [ 201].
Transformer Encoder
Super Bowl 50 was[MASK][MASK][MASK][MASK]to determineanAmericanfootballgame1 2 3 4
Bidirectional
Encoder
A_C_EB D
Bidirectional
DecoderB D A C E
B D A C <s>
(a) BART(b) SpanBERT1x2x3x4x5x6x7x8x9x10x
Fig. 11. The architecture of BART [202] and SpanBERT [203].
BART [ 202] is a denoising autoencoder based on the Seq2Seq model, as shown in Fig. 11 (a).
The pre-training of BART consists of two steps. Firstly, it uses a noise function to destroy the text.
Secondly, the Seq2Seq model is used to reconstruct the original text. In various noise methods,
by randomly shuffling the order of the original sentence and then using the first new text filling
method to obtain optimal performance. The new text filling method is replacing the text fragment
with a single mask token. It uses only a specific masked token to indicate that a token is masked.
SpanBERT [ 203] is specially designed to better represent and predict spans of text, as shown in
Fig. 11 (b). It optimizes BERT from three aspects and achieves good results in multiple tasks such
as QA. The specific optimization is embodied in three aspects. Firstly, the span mask scheme is
proposed to mask a continuous paragraph of text randomly. Secondly, Span Boundary Objective
(SBO) is added to predict span by the token next to the span boundary to get the better performance
to finetune stage. Thirdly, the NSP pre-training task is removed.
ERNIE [ 204] is based on the method of knowledge enhancement. It learns the semantic relations
in the real world by modeling the prior semantic knowledge such as entity concepts in massive
datasets. Specifically, ERNIE enables the model to learn the semantic representation of complete
concepts by masking semantic units such as words and entities. It mainly consists of a Transformer
encoder and task embedding. In the Transformer encoder, the context information of each token
is captured by the self-attention mechanism, and the context representation is generated for
embedding. Task embedding is used for tasks with different characteristics.
2.2.7 GNN-based Methods. The DNN models like CNN get great performance on regular structure,
not for arbitrarily structured graphs. Some researchers study how to expand on arbitrarily structured
graphs [ 205,206]. With the increasing attention of Graph Neural Networks (GNNs), GNN-based
models [ 207,208] obtain excellent performance by encoding syntactic structure of sentences on
semantic role labeling task [ 209], relation classification task [ 210] and machine translation task
[211]. It turns text classification into a graph node classification task. We show a GCN model for text
classification with four input texts, as shown in Fig. 12. Firstly, the four input texts ùëá=[ùëá1,ùëá2,ùëá3,ùëá4]
and the words ùëã=[ùë•1,ùë•2,ùë•3,ùë•4,ùë•5,ùë•6]in the text, defined as nodes, are constructed into the graph
structures. The graph nodes are connected by bold black edges, which indicates document-word
edges and word-word edges. The weight of each word-word edge usually means their co-occurrence
frequency in the corpus. Then, the words and texts are represented through the hidden layer. Finally,
the label of all input texts can be predicted by the graph.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:17
T2
x6
T4T1x1
x5T3
Hidden Layer
T2
T3
T1
T4x4
x3 x2
initial graph
T2x3
x6
T4T1x1
x5T3
Class1
Class2x2
x4
learned graph
Relation annotations in initial graph: document-document document-word word-word
Fig. 12. The GNN-based model. The initial graph differently depending on how the graph is designed. We
give an example to establish edges between documents and documents, documents and sentences, and words
to words.
The GNN-based models can learn the syntactic structure of sentences, making some researchers
study using GNN for text classification. DGCNN [ 153] is a graph-CNN converting text to graph-of-
words, having the advantage of learning different levels of semantics with CNN models. Yao et al.
[155] propose the Text Graph Convolutional Network (TextGCN), which builds a heterogeneous
word text graph for a whole dataset and captures global word co-occurrence information. To enable
GNN-based models to underpin online testing, Huang et al. [ 159] build graphs for each text with
global parameter sharing, not a corpus-level graph structure, to help preserve global information
and reduce the burden. TextING [ 162] builds individual graphs for each document and learns
text-level word interactions by GNN to effectively produce embeddings for obscure words in the
new text.
Graph ATtention network (GAT) [ 212] employs masked self-attention layers by attending over
its neighbors. Thus, some GAT-based models are proposed to compute the hidden representations
of each node. The Heterogeneous Graph ATtention networks (HGAT) [ 213] with a dual-level
attention mechanism learns the importance of different neighboring nodes and node types in the
current node. The model propagates information on the graph and captures the relations to address
the semantic sparsity for semi-supervised short text classification. MAGNET [166] is proposed to
capture the correlation among the labels based on GATs, which learns the crucial dependencies
between the labels and generates classifiers by a feature matrix and a correlation matrix.
Event Prediction (EP) can be divided into generated event prediction and selective event prediction
(also known as script event prediction). EP, referring to scripted event prediction in this review,
infers the subsequent event according to the existing event context. Unlike other text classification
tasks, texts in EP are composed of a series of sequential subevents. Extracting features of the
relationship among such subevents is of critical importance. SGNN [ 214] is proposed to model
event interactions and learn better event representations by constructing an event graph to utilize
the event network information better. The model makes full use of dense event connections for the
EP task.
2.2.8 Others. In addition to all the above models, there are some other individual models. Here we
introduce some exciting models.
Siamese Neural Network. The siamese neural network [ 215] is also called a twin neural network
(Twin NN). It utilizes equal weights while working in tandem using two distinct input vectors to
calculate comparable output vectors. Mueller et al. [ 216] present a siamese adaptation of the LSTM
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:18 Qian Li, et al.
network comprised of couples of variable-length sequences. The model is employed to estimate the
semantic similarity among texts, exceeding carefully handcrafted features and proposed neural
network models of higher complexity. The model further represents text employing neural networks
whose inputs are word vectors learned separately from a vast dataset. To settle unbalanced data
classification in the medical domain, Jayadeva et al. [ 217] use a Twin NN model to learn from
enormous unbalanced corpora. The objective functions achieve the Twin SVM approach with non-
parallel decision boundaries for the corresponding classes, and decrease the Twin NN complexity,
optimizing the feature map to better discriminate among classes.
Virtual Adversarial Training (VAT). Deep learning methods require many extra hyperparameters,
which increase the computational complexity. VAT [ 218], regularization based on local distributional
smoothness can be used in semi-supervised tasks, requires only some hyperparameters, and can be
interpreted directly as robust optimization. Miyato et al. [ 85] use VAT to effectively improve the
robustness and generalization ability of the model and word embedding performance.
Reinforcement Learning (RL). RL learns the best action in a given environment through maximiz-
ing cumulative rewards. Zhang et al. [ 219] offer an RL approach to establish structured sentence
representations via learning the structures related to tasks. The model has Information Distilled
LSTM (ID-LSTM) and Hierarchical Structured LSTM (HS-LSTM) representation models. The ID-
LSTM learns the sentence representation by choosing essential words relevant to tasks, and the
HS-LSTM is a two-level LSTM for modeling sentence representation.
Memory Networks. Memory networks [ 220] learn to combine the inference components and the
long-term memory component. Li et al. [ 221] use two LSTMs with extended memories and neural
memory operations for jointly handling the extraction tasks of aspects and opinions via memory
interactions. Topic Memory Networks (TMN) [ 169] is an end-to-end model that encodes latent
topic representations indicative of class labels.
QA Style for Sentiment Classification Task. It is an interesting attempt to treat the sentiment
classification task as a QA task. Shen et al. [ 222] create a high-quality annotated corpus. A three-
stage hierarchical matching network was proposed to consider the matching information between
questions and answers.
External Commonsense Knowledge. Due to the insufficient information of the event itself to
distinguish the event for the EP task, Ding et al. [ 223] consider that the event extracted from the
original text lacked common knowledge, such as the intention and emotion of the event participants.
The model improves the effect of stock prediction, EP, and so on.
Quantum Language Model. In the quantum language model, the words and dependencies among
words are represented through fundamental quantum events. Zhang et al. [ 224] design a quantum-
inspired sentiment representation method to learn both the semantic and the sentiment information
of subjective text. By inputting density matrices to the embedding layer, the performance of the
model improves.
Summary. RNN computes sequentially and cannot be calculated in parallel. The shortcoming
of RNN makes it more challenging to become mainstream in the current trend that models tend to
have deeper and more parameters. CNN extracts features from text vectors through the convolution
kernel. The number of features captured by the convolution kernel is related to its size. CNN is deep
enough that, in theory, it can capture features at long distances. Due to insufficient optimization
methods for parameters of the deep network and the loss of location information due to the pooling
layer, the deeper layer does not bring significant improvement. Compared with RNN, CNN has
parallel computing capability and can effectively retain location information for the improved
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:19
version of CNN. Still, it has weak feature capture capability for long-distance. GNN builds a graph
for text. When a valid graph structure is designed, the learned representation can better capture the
structural information. Transformer treats the input text as a fully connected graph, with attention
score weights on the edges. It is capable of parallel computing and is highly efficient in extracting
features between different words by self-attention, solving short-term memory problems. However,
the attention mechanism in Transformer is computation-heavy, especially when dealing with
long sequences. Some improved models [ 146,225] for computing complexity in Transformer have
recently been proposed. Overall, Transformer is a better choice for text classification. Deep Learning
consists of multiple hidden layers in a neural network with a higher level of complexity and can
be trained on unstructured data. Deep learning can learn language features and master higher
level and more abstract language features based on words and vectors. Deep learning architecture
can learn feature representations directly from the input without too many manual interventions
and prior knowledge. However, deep learning technology is a data-driven method that requires
enormous data to achieve high performance. Although self-attention based models can bring some
interpretability among words for DNNs, it is not enough comparing with traditional models to
explain why and how it works well.
3 DATASETS AND EVALUATION METRICS
3.1 Datasets
The availability of labeled datasets for text classification has become the main driving force behind
the fast advancement of this research field. In this section, we summarize the characteristics of
these datasets in terms of domains and give an overview in Table 2, including the number of
categories, average sentence length, the size of each dataset, related papers, data sources to access
and applications.
3.1.1 Sentiment Analysis (SA). SA is the process of analyzing and reasoning the subjective text
within emotional color. It is crucial to get information on whether it supports a particular point
of view from the text that is distinct from the traditional text classification that analyzes the
objective content of the text. SA can be binary or multi-class. Binary SA is to divide the text into
two categories, including positive and negative. Multi-class SA classifies text to multi-level or
fine-grained labels. The SA datasets include Movie Review (MR) [ 226,257], Stanford Sentiment
Treebank (SST) [ 227], Multi-Perspective Question Answering (MPQA) [ 229,258], IMDB [ 230], Yelp
[231], Amazon Reviews (AM) [ 93], NLP&CC 2013 [ 111], Subj [ 250], CR [ 251], SS-Twitter [ 259],
SS-Youtube [259], SE1604 [260] and so on. Here we detail several datasets.
MR. The MR is a movie review dataset, each of which corresponds to a sentence. The corpus
has 5,331 positive data and 5,331 negative data. 10-fold cross-validation by random splitting is
commonly used to test MR.
SST. The SST is an extension of MR. It has two categories. SST-1 with fine-grained labels with
five classes. It has 8,544 training texts and 2,210 test texts, respectively. Furthermore, SST-2 has
9,613 texts with binary labels being partitioned into 6,920 training texts, 872 development texts,
and 1,821 testing texts.
MPQA. The MPQA is an opinion dataset. It has two class labels and also an MPQA dataset of
opinion polarity detection sub-tasks. MPQA includes 10,606 sentences extracted from news articles
from various news sources. It should be noted that it contains 3,311 positive texts and 7,293 negative
texts without labels of each text.
IMDB reviews. The IMDB review is developed for binary sentiment classification of film reviews
with the same amount in each class. It can be separated into training and test groups on average,
by 25,000 comments per group.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:20 Qian Li, et al.
Table 2. Summary statistics for the datasets. C: Number of target classes. L: Average sentence length. N:
Dataset size.
Datasets #C #L #N Language Related Papers Sources Applications
MR 2 20 10,662 English [5, 17, 101, 155] [226] SA
SST-1 5 18 11,855 English [17, 64] [1, 2][112] [227] SA
SST-2 2 19 9,613 English [6, 17, 64] [19, 79] [228] SA
MPQA 2 3 10,606 English [17, 60, 120] [229] SA
IMDB 2 294 50,000 English [69][108] [6] [79] [85] [230] SA
Yelp.P 2 153 598,000 English [93, 98] [231] SA
Yelp.F 5 155 700,000 English [93, 98, 108] [231] SA
Amz.P 2 91 4,000,000 English [93, 122] [232] SA
Amz.F 5 93 3,650,000 English [93, 108, 122] [232] SA
Twitter 3 19 11,209 English [5][100] [233] SA
NLP&CC 2013 2 - 115,606 Multi-language [110] [111] SA
20NG 20 221 18,846 English [72, 75, 106, 155, 157] [34] NC
AG News 4 45/7 127,600 English [98, 100] [101, 138] [234] NC
R8 8 66 7,674 English [155, 157] [159] [235] NC
R52 52 70 9,100 English [155, 157] [159] [235] NC
Sogou 6 578 510,000 Chinese [93] [236] NC
Newsgroup 20 - 18,846 English [237] [237] NC
DBPedia 14 55 630,000 English [85, 93, 98, 136] [238] TL
Ohsumed 23 136 7,400 English [155, 157, 159] [239] TL
YahooA 10 112 1,460,000 English [93, 108] [93] TL
EUR-Lex 3,956 1,239 19,314 English [96] [134, 161] [134] [240] TL
Amazon670K 670 244 643,474 English [103, 122] [241] TL
Google news 152 6 11,109 English [3, 242, 243] [242] TL
TweetSet 2011-2012 89 - 2,472 English [242, 243] [242] TL
TweetSet 2011-2015 269 8 30,322 English [3, 4] [4] TL
Bing 4 20 34,871 English [100] [244] TL
Fudan 20 2981 18,655 Chinese [72] [245] TL
SQuAD - 5,000 5,570 English [118, 118, 140, 146] [246] QA
TREC-QA - 1,162 68 English [197] [247] QA
TREC 6 10 5,952 English [5, 6, 17] [100] [248] QA
WikiQA - 873 243 English [197, 249] [249] QA
Subj 2 23 10,000 English [17, 79, 101] [250] QA
CR 2 19 3,775 English [17, 101] [251] QA
Reuters 90 168 10,788 English [101, 166] [252] ML
Reuters10 10 168 9,979 English [253] [254] ML
RCV1 103 240 807,595 English [75, 103, 134, 153] [255] ML
RCV1-V2 103 124 804,414 English [116, 166] [256] ML
AAPD 54 163 55,840 English [116, 166] [117] ML
Yelp reviews. The Yelp review is summarized from the Yelp Dataset Challenges in 2013, 2014,
and 2015. This dataset has two categories. Yelp-2 of these were used for negative and positive
emotion classification tasks, including 560,000 training texts and 38,000 test texts. Yelp-5 is used to
detect fine-grained affective labels with 650,000 training and 50,000 test texts in all classes.
AM. The AM is a popular corpus formed by collecting Amazon website product reviews [ 232].
This dataset has two categories. The Amazon-2 with two classes includes 3,600,000 training sets
and 400,000 testing sets. Amazon-5, with five classes, includes 3,000,000 and 650,000 comments for
training and testing.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:21
3.1.2 News Classification (NC). News content is one of the most crucial information sources which
has a critical influence on people. The NC system facilitates users to get vital knowledge in real-time.
News classification applications mainly encompass: recognizing news topics and recommending
related news according to user interest. The news classification datasets include 20 Newsgroups
(20NG) [ 34], AG News (AG) [ 93,234], R8 [ 235], R52 [ 235], Sogou News (Sogou) [ 136] and so on.
Here we detail several datasets.
20NG. The 20NG is a newsgroup text dataset. It has 20 categories with the same number of each
category and includes 18,846 texts.
AG. The AG News is a search engine for news from academia, choosing the four largest classes.
It uses the title and description fields of each news. AG contains 120,000 texts for training and 7,600
texts for testing.
R8 and R52. R8 and R52 are two subsets which are the subset of Reuters [ 252]. R8 has 8 categories,
divided into 2,189 test files and 5,485 training courses. R52 has 52 categories, split into 6,532 training
files and 2,568 test files.
Sogou. The Sogou combines two datasets, including SogouCA and SogouCS news sets. The label
of each text is the domain names in the URL.
3.1.3 Topic Labeling (TL). The topic analysis attempts to get the meaning of the text by defining
the sophisticated text theme. The topic labeling is one of the essential components of the topic
analysis technique, intending to assign one or more subjects for each document to simplify the
topic analysis. The topic labeling datasets include DBPedia [ 238], Ohsumed [ 239], Yahoo answers
(YahooA) [ 93], EUR-Lex [ 240], Amazon670K [ 241], Bing [ 244], Fudan [ 245], and PubMed [ 261].
Here we detail several datasets.
DBpedia. The DBpedia is a large-scale multi-lingual knowledge base generated using Wikipedia‚Äôs
most ordinarily used infoboxes. It publishes DBpedia each month, adding or deleting classes and
properties in every version. DBpedia‚Äôs most prevalent version has 14 classes and is divided into
560,000 training data and 70,000 test data.
Ohsumed. The Ohsumed belongs to the MEDLINE database. It includes 7,400 texts and has 23
cardiovascular disease categories. All texts are medical abstracts and are labeled into one or more
classes.
YahooA. The YahooA is a topic labeling task with 10 classes. It includes 140,000 training data
and 5,000 test data. All text contains three elements, being question titles, question contexts, and
best answers, respectively.
3.1.4 Question Answering (QA). The QA task can be divided into two types: the extractive QA and
the generative QA. The extractive QA gives multiple candidate answers for each question to choose
which one is the right answer. Thus, the text classification models can be used for the extractive
QA task. The QA discussed in this paper is all extractive QA. The QA system can apply the text
classification model to recognize the correct answer and set others as candidates. The question
answering datasets include Stanford Question Answering Dataset (SQuAD) [ 246], TREC-QA [ 248],
WikiQA [ 249], Subj [ 250], CR [ 251], MS MARCO [ 262], and Quora [ 263]. Here we detail several
datasets.
SQuAD. The SQuAD is a set of question and answer pairs obtained from Wikipedia articles.
The SQuAD has two categories. SQuAD1.1 contains 536 pairs of 107,785 Q&A items. SQuAD2.0
combines 100,000 questions in SQuAD1.1 with more than 50,000 unanswerable questions that
crowd workers face in a form similar to answerable questions [264].
TREC-QA. The TREC-QA includes 5,452 training texts and 500 testing texts. It has two versions.
TREC-6 contains 6 categories, and TREC-50 has 50 categories.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:22 Qian Li, et al.
WikiQA. The WikiQA dataset includes questions with no correct answer, which needs to
evaluate the answer.
MS MARCO. The MS MARCO contains questions and answers. The questions and part of the
answers are sampled from actual web texts by the Bing search engine. Others are generative. It is
used for developing generative QA systems released by Microsoft.
3.1.5 Natural Language Inference (NLI). NLI is used to predict whether the meaning of one text can
be deduced from another. Paraphrasing is a generalized form of NLI. It uses the task of measuring the
semantic similarity of sentence pairs to decide whether one sentence is the interpretation of another.
The NLI datasets include Stanford Natural Language Inference (SNLI) [ 181], Multi-Genre Natural
Language Inference (MNLI) [ 265], Sentences Involving Compositional Knowledge (SICK) [ 266],
Microsoft Research Paraphrase (MSRP) [ 267], Semantic Textual Similarity (STS) [ 268], Recognising
Textual Entailment (RTE) [269], SciTail [270], etc. Here we detail several of the primary datasets.
SNLI. The SNLI is generally applied to NLI tasks. It contains 570,152 human-annotated sentence
pairs, including training, development, and test sets, which are annotated with three categories:
neutral, entailment, and contradiction.
MNLI. The MNLI is an expansion of SNLI, embracing a broader scope of written and spoken
text genres. It includes 433,000 sentence pairs annotated by textual entailment labels.
SICK. The SICK contains almost 10,000 English sentence pairs. It consists of neutral, entailment
and contradictory labels.
MSRP. The MSRP consists of sentence pairs, usually for the text-similarity task. Each pair is
annotated by a binary label to discriminate whether they are paraphrases. It respectively includes
1,725 training and 4,076 test sets.
3.1.6 Multi-Label (ML) datasets. In multi-label classification, an instance has multiple labels, and
each label can only take one of the multiple classes. There are many datasets based on multi-label
text classification. It includes Reuters [ 252], Reuters Corpus Volume I (RCV1) [ 255], RCV1-2K
[255], Arxiv Academic Paper Dataset (AAPD) [ 117], Patent, Web of Science (WOS-11967) [ 271],
AmazonCat-13K [272], BlurbGenreCollection (BGC) [273], etc. Here we detail several datasets.
Reuters. The Reuters is a popularly used dataset for text classification from Reuters financial
news services. It has 90 training classes, 7,769 training texts, and 3,019 testing texts, containing
multiple labels and single labels. There are also some Reuters sub-sets of data, such as R8, BR52,
RCV1, and RCV1-v2.
RCV1 and RCV1-2K. The RCV1 is collected from Reuters News articles from 1996-1997, which
is human-labeled with 103 categories. It consists of 23,149 training and 784,446 testing texts,
respectively. The RCV1-2K dataset has the same features as the RCV1. However, the label set of
RCV1-2K has been expanded with some new labels. It contains 2456 labels.
AAPD. The AAPD is a large dataset in the computer science field for the multi-label text
classification from website1. It has 55,840 papers, including the abstract and the corresponding
subjects with 54 labels in total. The aim is to predict the corresponding subjects of each paper
according to the abstract.
Patent Dataset. The Patent Dataset is obtained from USPTO2, which is a patent system grating
U.S. patents containing textual details such title and abstract. It contains 100,000 US patents awarded
in the real-world with multiple hierarchical categories.
1https://arxiv.org/
2https://www.uspto.gov/
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:23
Table 3. The notations used in evaluation metrics.
Notations Descriptions
ùëáùëÉ true positive
ùêπùëÉ false positive
ùëáùëÅ true negative
ùêπùëÅ false negative
ùëáùëÉùë° true positive of the ùë°th label on a text
ùêπùëÉùë° false positive of the ùë°th label on a text
ùëáùëÅùë° true negative of the ùë°th label on a text
ùêπùëÅùë° false negative of the ùë°th label on a text
S label set of all samples
ùëÑ the number of predicted labels on each text
WOS-11967. The WOS-11967 is crawled from the Web of Science, consisting of abstracts of
published papers with two labels for each example. It is shallower, but significantly broader, with
fewer classes in total.
3.1.7 Others. There are some datasets for other applications, such as SemEval-2010 Task 8 [ 274],
ACE 2003-2004 [ 275], TACRED [ 276], and NYT-10 [ 277], FewRel [ 278], Dialog State Tracking
Challenge 4 (DSTC 4) [ 279], ICSI Meeting Recorder Dialog Act (MRDA) [ 280], and Switchboard
Dialog Act (SwDA) [281], and so on.
3.2 Evaluation Metrics
In terms of evaluating text classification models, accuracy and F1 score are the most used to assess
the text classification methods. Later, with the increasing difficulty of classification tasks or the
existence of some particular tasks, the evaluation metrics are improved. For example, evaluation
metrics such as ùëÉ@ùêæandùëÄùëñùëêùëüùëú‚àíùêπ1are used to evaluate multi-label text classification performance,
and MRR is usually used to estimate the performance of QA tasks. In Table 3, we give the notations
used in evaluation metrics.
3.2.1 Single-label metrics. Single-label text classification divides the text into one of the most likely
categories applied in NLP tasks such as QA, SA, and dialogue systems [ 7]. For single-label text
classification, one text belongs to just one catalog, making it possible not to consider the relations
among labels. Here, we introduce some evaluation metrics used for single-label text classification
tasks.
Accuracy and ErrorRate. The Accuracy and ErrorRate are the fundamental metrics for a text
classification model. The ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ andùê∏ùëüùëüùëúùëüùëÖùëéùë°ùëí are respectively defined as
ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ =(ùëáùëÉ+ùëáùëÅ)
ùëÅ, (1)
ùê∏ùëüùëüùëúùëüùëÖùëéùë°ùëí =1‚àíùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ =(ùêπùëÉ+ùêπùëÅ)
ùëÅ. (2)
Precision, Recall and F1. These are vital metrics utilized for unbalanced test sets, regardless of
the standard type and error rate. For example, most of the test samples have a class label. ùêπ1is the
harmonic average of ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ andùëÖùëíùëêùëéùëôùëô .ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ ,ùëÖùëíùëêùëéùëôùëô , andùêπ1as defined
ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ =ùëáùëÉ
ùëáùëÉ+ùêπùëÉ, ùëÖùëíùëêùëéùëôùëô =ùëáùëÉ
ùëáùëÉ+ùêπùëÅ, (3)
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:24 Qian Li, et al.
ùêπ1=2ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ√óùëÖùëíùëêùëéùëôùëô
ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ+ùëÖùëíùëêùëéùëôùëô. (4)
The desired results will be obtained when the accuracy, ùêπ1andùëÖùëíùëêùëéùëôùëô value reach 1. On the contrary,
when the values become 0, the worst result is obtained. For the multi-class classification problem,
the precision and recall value of each class can be calculated separately, and then the performance
of the individual and whole can be analyzed.
Exact Match (EM). The EM [ 29] is a metric for QA tasks, measuring the prediction that matches
all the ground-truth answers precisely. It is the primary metric utilized on the SQuAD dataset.
Mean Reciprocal Rank (MRR). The MRR [ 282] is usually applied for assessing the performance
of ranking algorithms on QA and Information Retrieval (IR) tasks. ùëÄùëÖùëÖ is defined as
ùëÄùëÖùëÖ =1
ùëÑùëÑ‚àëÔ∏Å
ùëñ=11
ùëüùëéùëõùëò(ùëñ), (5)
whereùëüùëéùëõùëò(ùëñ)is the ranking of the ground-truth answer at answer ùëñ-th.
Hamming-Loss (HL). The HL [ 57] assesses the score of misclassified instance-label pairs where
a related label is omitted or an unrelated is predicted.
Among these single-label evaluation metrics, the Accuracy is the earliest metric that calculates
the proportion of the sample size that is predicted correctly and is not considered whether the
predicted sample is a positive or a negative sample. ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ calculates how many of the positive
samples are actually positive, and the ùëÖùëíùëêùëéùëôùëô calculates how many of the positive examples in the
sample are predicted correctly. Furthermore, ùêπ1is the harmonic average of them, which is the most
commonly used evaluation metrics.
3.2.2 Multi-label metrics. Compared with single-label text classification, multi-label text classifica-
tion divides the text into multiple category labels, and the number of category labels is variable.
These metrics are designed for single label text classification, which are not suitable for multi-label
tasks. Thus, there are some metrics designed for multi-label text classification.
ùë¥ùíäùíÑùíìùíê ‚àíùë≠1.TheùëÄùëñùëêùëüùëú‚àíùêπ1[283] is a measure that considers the overall accuracy and recall of all
labels. TheùëÄùëñùëêùëüùëú‚àíùêπ1is defined as:
ùëÄùëñùëêùëüùëú‚àíùêπ1=2ùëÉùë°√óùëÖùë°
ùëÉ+ùëÖ, (6)
where:
ùëÉ=√ç
ùë°‚ààSùëáùëÉùë°√ç
ùë°‚ààùëÜùëáùëÉùë°+ùêπùëÉùë°, ùëÖ =√ç
ùë°‚ààùëÜùëáùëÉùë°√ç
ùë°‚ààSùëáùëÉùë°+ùêπùëÅùë°. (7)
ùë¥ùíÇùíÑùíìùíê ‚àíùë≠1.TheùëÄùëéùëêùëüùëú‚àíùêπ1[283] calculates the average ùêπ1of all labels. Unlike ùëÄùëñùëêùëüùëú‚àíùêπ1, which
sets even weight to every example, ùëÄùëéùëêùëüùëú‚àíùêπ1sets the same weight to all labels in the average
process. Formally, ùëÄùëéùëêùëüùëú‚àíùêπ1is defined as:
ùëÄùëéùëêùëüùëú‚àíùêπ1=1
S‚àëÔ∏Å
ùë°‚ààS2ùëÉùë°√óùëÖùë°
ùëÉùë°+ùëÖùë°, (8)
where:
ùëÉùë°=ùëáùëÉùë°
ùëáùëÉùë°+ùêπùëÉùë°, ùëÖ ùë°=ùëáùëÉùë°
ùëáùëÉùë°+ùêπùëÅùë°. (9)
In addition to the above evaluation metrics, there are some rank-based evaluation metrics for
extreme multi-label classification tasks, including ùëÉ@ùêæandùëÅùê∑ùê∂ùê∫ @ùêæ.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:25
Precision at Top K (P@K). TheùëÉ@ùêæ[96] is the precision at the top k. For ùëÉ@ùêæ, each text has
a set ofLground truth labels ùêøùë°={ùëô0,ùëô1,ùëô2...,ùëôL‚àí1}, in order of decreasing probability ùëÉùë°=
ùëù0,ùëù1,ùëù2...,ùëù ùëÑ‚àí1
.The precision at ùëòis
ùëÉ@ùêæ=1
ùëòmin(L,ùëò)‚àí1‚àëÔ∏Å
ùëó=0ùëüùëíùëôùêøùëñ(ùëÉùë°(ùëó)), (10)
relùêø(ùëù)=(
1ifùëù‚ààùêø
0otherwise, (11)
whereLis the number of ground truth labels or possible answers on each text and ùëòis the number
of selected labels on extreme multi-label text classification.
Normalized Discounted Cummulated Gains (NDCG@K). TheùëÅùê∑ùê∂ùê∫ @ùêæ[96] is
ùëÅùê∑ùê∂ùê∫ @ùêæ=1
ùêºùê∑ùê∂ùê∫(ùêøùëñ,ùëò)ùëõ‚àí1‚àëÔ∏Å
ùëó=0ùëüùëíùëôùêøùëñ(ùëÉùë°(ùëó))
ln(ùëó+1), (12)
whereùêºùê∑ùê∂ùê∫ is ideal discounted cumulative gain and the particular rank position ùëõis
ùëõ=min(max(|ùëÉùëñ|,|ùêøùëñ|),ùëò). (13)
Among these multi-label evaluation metrics, ùëÄùëñùëêùëüùëú‚àíùêπ1considers the number of categories, which
makes it suitable for the unbalanced data distribution. ùëÄùëéùëêùëüùëú‚àíùêπ1does not take into account the
amount of data that treats each class equally. Thus, it is easily affected by the classes with high
Recall and Precision. When the number of categories is large or extremely large, either P@K or
NDCG@K is used.
4 QUANTITATIVE RESULTS
There are many differences between sentiment analysis, news classification, topic labeling and
natural language inference tasks, which can not be simplified modeled as a text classification task.
In this section, we tabulate the performance of the main models given in their articles on classic
datasets evaluated by classification accuracy, as shown in Table 4, including MR, SST-2, IMDB,
Yelp.P, Yelp.F, Amazon.F, 20NG, AG, DBpedia, and SNLI.
We give the performance of NB and SVM algorithms from RNTN [ 64] due to the less traditional
text classification model has been an experiment on datasets in Table 4. The accuracy of NB and
SVM are 81.8% and 79.4% on SST-2, respectively. We can see that, in the SST-2 data set with only
two categories, the accuracy of NB is better than that of SVM. It may be because NB has relatively
stable classification efficiency on new data sets. The performance is also stable on small data sets.
Compared with the deep learning model, the performance of NB is lower. NB has the advantage
of lower computational complexity than deep models. However, it requires manual classification
features, making it difficult to migrate the model directly to other data sets.
For deep learning models, pre-trained models get better results on most datasets. It means
that if you need to implement a text classification task, you can preferentially try pre-trained
models, such as BERT, RoBERTa, and XLNET, etc., except MR and 20NG, which have not been
experimented on BERT based models. Pre-trained models are essential to NLP. It uses a deep model
to learn a better feature of the text. It also demonstrates that the accuracy of NLP tasks can be
significantly improved by a profound model that can be pre-trained from unlabeled datasets. For the
MR dataset, the accuracy of RNN-Capsule [ 87] is 83.8%, obtaining the best result. It suggests that
RNN-Capsule builds a capsule in each category for sentiment analysis. It can output words including
sentiment trends indicating attributes of capsules with no applying linguistic knowledge. For 20NG
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:26 Qian Li, et al.
Table 4. Accuracy of text classification models on primary datasets evaluated by classification accuracy (in
terms of publication year). Bold is the most accurate.
ModelSentiment News Topic NLI
MR SST-2 IMDB Yelp.P Yelp.F Amz.F 20NG AG DBpedia SNLI
NB [8] - 81.80 - - - - - - - -
SVM [42] - 79.40 - - - - - - - -
Tree-CRF [284] 77.30 - - - - - - - - -
RAE [60] 77.70 82.40 - - - - - - - -
MV-RNN [62] 79.00 82.90 - - - - - - - -
RNTN [64] 75.90 85.40 - - - - - - - -
DCNN [5] 86.80 89.40 - - - - - - -
Paragraph-Vec [67] 87.80 92.58 - - - - - - -
TextCNN[17] 81.50 88.10 - - - - - - - -
TextRCNN [72] - - - - - - 96.49 - - -
DAN [69] - 86.30 89.40 - - - - - - -
Tree-LSTM [1] 88.00 - - - - - - - -
CharCNN [93] - - - 95.12 62.05 - - 90.49 98.45 -
HAN [108] - - 49.40 - - 63.60 - - - -
SeqTextRCNN [7] - - - - - - - - - -
oh-2LSTMp [75] - - 94.10 97.10 67.61 - 86.68 93.43 99.16 -
LSTMN [112] - 87.30 - - - - - - - -
Multi-Task [79] - 87.90 91.30 - - - - - - -
BLSTM-2DCNN [77] 82.30 89.50 - - - - 96.50 - - -
TopicRNN [83] - - 93.72 - - - - - - -
DPCNN [98] - - - 97.36 69.42 65.19 - 93.13 99.12 -
KPCNN [100] 83.25 - - - - - - 88.36 - -
RNN-Capsule [87] 83.80 - - - - - - - -
ULMFiT [285] - - 95.40 97.84 71.02 - - 94.99 99.20 -
LEAM[286] 76.95 - - 95.31 64.09 - 81.91 92.45 99.02 -
TextCapsule [101] 82.30 86.80 - - - - - 92.60 - -
TextGCN [155] 76.74 - - - - - 86.34 67.61 - -
BERT-base [19] - 93.50 95.63 98.08 70.58 61.60 - - - 91.00
BERT-large [19] - 94.90 95.79 98.19 71.38 62.20 - - - 91.70
MT-DNN[287] - 95.60 83.20 - - - - - - 91.50
XLNet-Large [138] - 96.80 96.21 98.45 72.20 67.74 - - - -
XLNet [138] - 97.00 - - - - - 95.51 99.38 -
RoBERTa [140] - 96.40 - - - - - - - 92.60
dataset, BLSTM-2DCNN [ 77] gets 96.5% score with the best accuracy score. It may demonstrate the
effectiveness of applying the 2D max-pooling operation to obtain a fixed-length representation of
the text and utilize 2D convolution to sample more meaningful matrix information.
5 FUTURE RESEARCH CHALLENGES
Text classification ‚Äì as efficient information retrieval and mining technology ‚Äì plays a vital role in
managing text data. It uses NLP, data mining, machine learning, and other techniques to automati-
cally classify and discover different text types. Text classification takes multiple types of text as
input, and the text is represented as a vector by the pre-training model. Then the vector is fed into
the DNN for training until the termination condition is reached, and finally, the performance of
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:27
the training model is verified by the downstream task. Existing models have already shown their
usefulness in text classification, but there are still many possible improvements to explore.
Although some new text classification models repeatedly brush up the accuracy index of most
classification tasks, it cannot indicate whether the model "understands" the text from the semantic
level like human beings. Moreover, with the emergence of the noise sample, the small sample
noise may cause the decision confidence to change substantially or even lead to decision reversal.
Therefore, the semantic representation ability and robustness of the model need to be proved in
practice. Besides, the pre-trained semantic representation model represented by word vectors can
often improve the performance of downstream NLP tasks. The existing research on the transfer
strategy of context-free word vectors is still relatively preliminary. Thus, we conclude from data,
models, and performance perspective, text classification mainly faces the following challenges.
5.1 Challenges from Data Perspective
For a text classification task, data is essential to model performance, whether it is traditional or deep
learning method. The text data mainly studied includes multi-chapter, short text, cross-language,
multi-label, less sample text. For the characteristics of these data, the existing technical challenges
are as follows:
Zero-shot/Few-shot learning. Zero-shot or few-shot learning for text classification aim to classify
text having no or few same labeled class data. However, the current models are too dependent on
numerous labeled data. The performance of these models is significantly affected by zero-shot or
few-shot learning. Thus, some works focus on tackling these problems. The main idea is to infer the
features through learning kinds of semantic knowledge, such as learning relationship among classes
[288] and incorporating class descriptions [ 170]. Furthermore, latent features generation [ 289]
meta-Learning [ 106,290,291] and dynamic memory mechanism [ 292] are also efficient methods.
Nevertheless, with the limitation of little unseen class data and different data distribution between
seen class and unseen class, there is still a long way to go to reach the learning ability comparable
to that of humans.
The external knowledge. As we all know, the more beneficial information is input into a DNN,
its better performance. For example, a question answering system incorporating a common-sense
knowledge base can answer questions about the real world and help solve problems with incomplete
information. Therefore, adding external knowledge (knowledge base or knowledge graph) [ 293,294]
is an efficient way to promote the model‚Äôs performance. The existing knowledge includes conceptual
information [ 100,127,204], commonsense knowledge [ 223], knowledge base information [ 295,296],
general knowledge graph [ 170] and so on, which enhances the semantic representation of texts.
Nevertheless, with the limitation of input scale, how and what to add for different tasks is still a
challenge.
Special domain with many terminologies. Most of the existing models are supervised models,
which over-rely on numerous labeled data. When the sample size is too small, or zero samples
occur, the performance of the model will be significantly affected. New data set annotation takes a
lot of time. Therefore, unsupervised learning and semi-supervised learning have great potential for
text classification. Furthermore, texts in a particular field [ 297,298], such as financial and medical
texts, contain many specific words or domain experts intelligible slang, abbreviations, etc., which
make the existing pre-trained word vectors challenging to work on.
The multi-label text classification task. Multi-label text classification requires full consideration
of the semantic relationship among labels, and the embedding and encoding of the model is a
process of lossy compression [ 299,300]. Therefore, how to reduce the loss of hierarchical semantics
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:28 Qian Li, et al.
and retain rich and complex document semantic information during training is still a problem to
be solved.
5.2 Challenges from Model Perspective
Most existing structures of traditional and deep learning models are tried for text classification,
including integration methods. BERT learns a language representation that can be used to fine-tune
for many NLP tasks. The primary method is to increase data, improve computation power, and
design training procedures for getting better results [ 301‚Äì303]. How the tradeoff between data and
compute resources and prediction performance is worth studying.
Text representation. The text representation method based on the vector space model is simple
and effective in the text preprocessing stage. However, it will lose the semantic information of the
text, so the application performance based on this method is limited. The proposed semantically
based text representation method is too time-consuming. Therefore, the efficient semantically
based text representation method still needs further research. In the text representation of text
classification based on deep learning, word embedding is the main concept, while the representation
unit is described differently in different languages. Then, a word is represented in the form of a vector
by learning mapping rules through the model. Therefore, how to design adaptive data representation
methods is more conducive to the combination of deep learning and specific classification tasks.
Model integration. Most structures of traditional and deep learning models are tried for text
classification, including integration methods. RNN requires recursive step by step to get global
information. CNN can obtain local information, and the sensing field can be increased through the
multi-layer stack to capture more comprehensive contextual information. Attention mechanisms
learn global dependency among words in a sentence. The transformer model is dependent on
attention mechanisms to establish the depth of the global dependency relationship between the
input and output. Therefore, designing an integrated model is worth trying to take advantage of
these models.
Model efficiency. Although text classification models based on deep learning are highly effective,
such as CNNs, RNNs, and GNNs. However, there are many technical limitations, such as the depth
of the network layer, regularization problem, network learning rate, etc. Therefore, there is still
more broad space for development to optimize the algorithm and improve the speed of model
training.
5.3 Challenges from Performance Perspective
The traditional model and the deep model can achieve good performance in most text classification
tasks, but the anti-interference ability of their results needs to be improved [ 176,304,305]. How to
realize the interpretation of the deep model is also a technical challenge.
The semantic robustness of the model. In recent years, researchers have designed many models
to enhance the accuracy of text classification models. However, when there are some adversarial
samples in the datasets, the model‚Äôs performance decreases significantly. Adversarial training is a
crucial method to improve the robustness of the pre-training model. For example, a popular approach
is converting attack into defense and using the adversarial sample training model. Consequently,
how to improve the robustness of models is a current research hotspot and challenge.
The interpretability of the model. DNNs have unique advantages in feature extraction and
semantic mining and have achieved excellent text classification tasks. Only a better understanding
of the theories behind these models can accurately design better models for various applications.
However, deep learning is a black-box model, the training process is challenging to reproduce,
and the implicit semantics and output interpretability are poor. It makes the improvement and
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:29
optimization of the model, losing clear guidelines. Why does one model outperform another on
one data set but underperform on others? What does the deep learning model learn? Furthermore,
we cannot accurately explain why the model improves performance.
6 CONCLUSION
This paper principally introduces the existing models for text classification tasks from traditional
models to deep learning. Firstly, we introduce some primary traditional models and deep learning
models with a summary table. The traditional model improves text classification performance
mainly by improving the feature extraction scheme and classifier design. In contrast, the deep
learning model enhances performance by improving the presentation learning method, model
structure, and additional data and knowledge. Then, we introduce the datasets with a summary
table and evaluation metrics for single-label and multi-label tasks. Furthermore, we give the
quantitative results of the leading models in a summary table under different applications for classic
text classification datasets. Finally, we summarize the possible future research challenges of text
classification.
ACKNOWLEDGMENTS
The authors of this paper were supported by the National Key R&D Program of China through
grant 2021YFB1714800, NSFC through grants (No.U20B2053 and 61872022), State Key Laboratory
of Software Development Environment (SKLSDE-2020ZX-12). Philip S. Yu was supported by NSF
under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941. Lifang He was supported
by NSF ONR N00014-18-1-2009 and Lehigh‚Äôs accelerator grant S00010293. This work was also
sponsored by CAAI-Huawei MindSpore Open Fund. Thanks for computing infrastructure provided
by Huawei MindSpore platform.
REFERENCES
[1]K. S. Tai, R. Socher, and C. D. Manning, ‚ÄúImproved semantic representations from tree-structured long short-term
memory networks,‚Äù in Proc. ACL, 2015 , pp. 1556‚Äì1566, 2015.
[2]X. Zhu, P. Sobhani, and H. Guo, ‚ÄúLong short-term memory over recursive structures, ‚Äù in Proc. ICML, 2015 , pp. 1604‚Äì1612,
2015.
[3]J. Chen, Z. Gong, and W. Liu, ‚ÄúA dirichlet process biterm-based mixture model for short text stream clustering,‚Äù Appl.
Intell. , vol. 50, no. 5, pp. 1609‚Äì1619, 2020.
[4]J. Chen, Z. Gong, and W. Liu, ‚ÄúA nonparametric model for online topic discovery with word embeddings,‚Äù Inf. Sci. ,
vol. 504, pp. 32‚Äì47, 2019.
[5]N. Kalchbrenner, E. Grefenstette, and P. Blunsom, ‚ÄúA convolutional neural network for modelling sentences,‚Äù in Proc.
ACL, 2014 , pp. 655‚Äì665, 2014.
[6]P. Liu, X. Qiu, X. Chen, S. Wu, and X. Huang, ‚ÄúMulti-timescale long short-term memory neural network for modelling
sentences and documents,‚Äù in Proc. EMNLP, 2015 , pp. 2326‚Äì2335, 2015.
[7]J. Y. Lee and F. Dernoncourt, ‚ÄúSequential short-text classification with recurrent and convolutional neural networks,‚Äù
inProc. NAACL, 2016 , pp. 515‚Äì520, 2016.
[8] M. E. Maron, ‚ÄúAutomatic indexing: An experimental inquiry,‚Äù J. ACM , vol. 8, no. 3, pp. 404‚Äì417, 1961.
[9]T. M. Cover and P. E. Hart, ‚ÄúNearest neighbor pattern classification,‚Äù IEEE Trans. Inf. Theory , vol. 13, no. 1, pp. 21‚Äì27,
1967.
[10] T. Joachims, ‚ÄúText categorization with support vector machines: Learning with many relevant features,‚Äù in Proc.
ECML, 1998 , pp. 137‚Äì142, 1998.
[11] R. Aly, S. Remus, and C. Biemann, ‚ÄúHierarchical multi-label classification of text with capsule networks,‚Äù in Proceedings
of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28 - August 2,
2019, Volume 2: Student Research Workshop , pp. 323‚Äì330, 2019.
[12] K. Kowsari, K. J. Meimandi, M. Heidarysafa, S. Mendu, L. E. Barnes, and D. E. Brown, ‚ÄúText classification algorithms:
A survey,‚Äù Information , vol. 10, no. 4, p. 150, 2019.
[13] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, and J. Gao, ‚ÄúDeep learning based text classification:
A comprehensive review,‚Äù CoRR , vol. abs/2004.03705, 2020.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:30 Qian Li, et al.
[14] L. Breiman, ‚ÄúRandom forests,‚Äù Mach. Learn. , vol. 45, no. 1, pp. 5‚Äì32, 2001.
[15] T. Chen and C. Guestrin, ‚ÄúXgboost: A scalable tree boosting system,‚Äù in Proc. ACM SIGKDD, 2016 , pp. 785‚Äì794, 2016.
[16] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, and T. Liu, ‚ÄúLightgbm: A highly efficient gradient boosting
decision tree,‚Äù in Proc. NeurIPS, 2017 , pp. 3146‚Äì3154, 2017.
[17] Y. Kim, ‚ÄúConvolutional neural networks for sentence classification,‚Äù in Proc. EMNLP, 2014 , pp. 1746‚Äì1751, 2014.
[18] S. Albawi, T. A. Mohammed, and S. Al-Zawi, ‚ÄúUnderstanding of a convolutional neural network, ‚Äù in 2017 International
Conference on Engineering and Technology (ICET) , pp. 1‚Äì6, Ieee, 2017.
[19] J. Devlin, M. Chang, K. Lee, and K. Toutanova, ‚ÄúBERT: pre-training of deep bidirectional transformers for language
understanding,‚Äù in Proc. NAACL, 2019 , pp. 4171‚Äì4186, 2019.
[20] Y. Zhang, R. Jin, and Z.-H. Zhou, ‚ÄúUnderstanding bag-of-words model: a statistical framework,‚Äù International Journal
of Machine Learning and Cybernetics , vol. 1, no. 1-4, pp. 43‚Äì52, 2010.
[21] W. B. Cavnar, J. M. Trenkle, et al., ‚ÄúN-gram-based text categorization, ‚Äù in Proceedings of SDAIR-94, 3rd annual symposium
on document analysis and information retrieval , vol. 161175, Citeseer, 1994.
[22] ‚ÄúTerm frequency by inverse document frequency,‚Äù in Encyclopedia of Database Systems , p. 3035, 2009.
[23] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfficient estimation of word representations in vector space,‚Äù in Proc.
ICLR, 2013 , 2013.
[24] J. Pennington, R. Socher, and C. D. Manning, ‚ÄúGlove: Global vectors for word representation,‚Äù in Proc. EMNLP, 2014 ,
pp. 1532‚Äì1543, 2014.
[25] M. Zhang and K. Zhang, ‚ÄúMulti-label learning by exploiting label dependency,‚Äù in Proc. ACM SIGKDD, 2010 , pp. 999‚Äì
1008, 2010.
[26] K. Schneider, ‚ÄúA new feature selection score for multinomial naive bayes text classification based on kl-divergence,‚Äù
inProc. ACL, 2004 , 2004.
[27] T. M. Cover and J. A. Thomas, Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing) .
USA: Wiley-Interscience, 2006.
[28] W. Dai, G. Xue, Q. Yang, and Y. Yu, ‚ÄúTransferring naive bayes classifiers for text classification,‚Äù in Proc. AAAI, 2007 ,
pp. 540‚Äì545, 2007.
[29] A., P., Dempster, N., M., Laird, D., B., and Rubin, ‚ÄúMaximum likelihood from incomplete data via the em algorithm,‚Äù
Journal of the Royal Statistical Society , 1977.
[30] M. Granik and V. Mesyura, ‚ÄúFake news detection using naive bayes classifier,‚Äù in 2017 IEEE First Ukraine Conference
on Electrical and Computer Engineering (UKRCON) , pp. 900‚Äì903, 2017.
[31] M. S. Mubarok, K. Adiwijaya, and M. Aldhi, ‚ÄúAspect-based sentiment analysis to review products using na√Øve bayes,‚Äù
vol. 1867, p. 020060, 08 2017.
[32] S. Xu, ‚ÄúBayesian na√Øve bayes classifiers to text classification,‚Äù J. Inf. Sci. , vol. 44, no. 1, pp. 48‚Äì59, 2018.
[33] G. Singh, B. Kumar, L. Gaur, and A. Tyagi, ‚ÄúComparison between multinomial and bernoulli na√Øve bayes for text
classification,‚Äù in 2019 International Conference on Automation, Computational and Technology Management (ICACTM) ,
pp. 593‚Äì596, 2019.
[34] ‚Äú20NG Corpus.‚Äù http://ana.cachopo.org/datasets-for-single-label-text-categorization, 2007.
[35] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. M. Mitchell, K. Nigam, and S. Slattery, ‚ÄúLearning to extract
symbolic knowledge from the world wide web,‚Äù in Proceedings of the Fifteenth National Conference on Artificial
Intelligence and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98, IAAI 98, July 26-30, 1998,
Madison, Wisconsin, USA , pp. 509‚Äì516, 1998.
[36] T. Jo, ‚ÄúUsing k nearest neighbors for text segmentation with feature similarity,‚Äù in 2017 International Conference on
Communication, Control, Computing and Electronics Engineering (ICCCCEE) , pp. 1‚Äì5, 2017.
[37] L. Baoli, L. Qin, and Y. Shiwen, ‚ÄúAn adaptive <i>k</i>-nearest neighbor text categorization strategy,‚Äù ACM Transactions
on Asian Language Information Processing , vol. 3, p. 215‚Äì226, Dec. 2004.
[38] S. Chen, ‚ÄúK-nearest neighbor algorithm optimization in text categorization,‚Äù IOP Conference Series: Earth and Environ-
mental Science , vol. 108, p. 052074, jan 2018.
[39] S. Jiang, G. Pang, M. Wu, and L. Kuang, ‚ÄúAn improved k-nearest-neighbor algorithm for text categorization,‚Äù Expert
Syst. Appl. , vol. 39, no. 1, pp. 1503‚Äì1509, 2012.
[40] P. Soucy and G. W. Mineau, ‚ÄúA simple KNN algorithm for text categorization,‚Äù in Proc. ICDM, 2001 , pp. 647‚Äì648, 2001.
[41] S. Tan, ‚ÄúNeighbor-weighted k-nearest neighbor for unbalanced text corpus,‚Äù Expert Syst. Appl. , vol. 28, no. 4, pp. 667‚Äì
671, 2005.
[42] C. Cortes and V. Vapnik, ‚ÄúSupport-vector networks,‚Äù Mach. Learn. , vol. 20, no. 3, pp. 273‚Äì297, 1995.
[43] C. Leslie, E. Eskin, and W. S. Noble, ‚ÄúThe spectrum kernel: A string kernel for svm protein classification,‚Äù in Biocom-
puting 2002 , pp. 564‚Äì575, World Scientific, 2001.
[44] H. Taira and M. Haruno, ‚ÄúFeature selection in svm text categorization,‚Äù in AAAI/IAAI , pp. 480‚Äì486, 1999.
[45] X. Li and Y. Guo, ‚ÄúActive learning with multi-label svm classification.,‚Äù in IjCAI , pp. 1479‚Äì1485, Citeseer, 2013.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:31
[46] T. Peng, W. Zuo, and F. He, ‚ÄúSvm based adaptive learning method for text classification from positive and unlabeled
documents,‚Äù Knowledge and Information Systems , vol. 16, no. 3, pp. 281‚Äì301, 2008.
[47] T. Joachims, ‚ÄúA statistical learning model of text classification for support vector machines,‚Äù in Proc. SIGIR, 2001 ,
pp. 128‚Äì136, 2001.
[48] T. JOACHIMS, ‚ÄúTransductive inference for text classification using support vector macines,‚Äù in International Conference
on Machine Learning , 1999.
[49] T. M. Mitchell, Machine learning . McGraw Hill series in computer science, McGraw-Hill, 1997.
[50] R. Rastogi and K. Shim, ‚ÄúPUBLIC: A decision tree classifier that integrates building and pruning,‚Äù Data Min. Knowl.
Discov. , vol. 4, no. 4, pp. 315‚Äì344, 2000.
[51] R. J. Quinlan, ‚ÄúInduction of decision trees,‚Äù Machine Learning , vol. 1, no. 1, pp. 81‚Äì106, 1986.
[52] J. R. Quinlan, C4.5: Programs for Machine Learning . San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1993.
[53] M. Kamber, L. Winstone, W. Gong, S. Cheng, and J. Han, ‚ÄúGeneralization and decision tree induction: efficient
classification in data mining,‚Äù in Proceedings Seventh International Workshop on Research Issues in Data Engineering.
High Performance Database Management for Large-Scale Applications , pp. 111‚Äì120, IEEE, 1997.
[54] D. E. Johnson, F. J. Oles, T. Zhang, and T. G√∂tz, ‚ÄúA decision-tree-based symbolic rule induction system for text
categorization,‚Äù IBM Syst. J. , vol. 41, no. 3, pp. 428‚Äì437, 2002.
[55] P. Vateekul and M. Kubat, ‚ÄúFast induction of multiple decision trees in text categorization from large scale, imbalanced,
and multi-label data,‚Äù in Proc. ICDM Workshops, 2009 , pp. 320‚Äì325, 2009.
[56] Y. Freund and R. E. Schapire, ‚ÄúA decision-theoretic generalization of on-line learning and an application to boosting,‚Äù
inProc. EuroCOLT, 1995 , pp. 23‚Äì37, 1995.
[57] R. E. Schapire and Y. Singer, ‚ÄúImproved boosting algorithms using confidence-rated predictions,‚Äù Mach. Learn. , vol. 37,
no. 3, pp. 297‚Äì336, 1999.
[58] A. Bouaziz, C. Dartigues-Pallez, C. da Costa Pereira, F. Precioso, and P. Lloret, ‚ÄúShort text classification using semantic
random forest,‚Äù in Proc. DAWAK, 2014 , pp. 288‚Äì299, 2014.
[59] M. Z. Islam, J. Liu, J. Li, L. Liu, and W. Kang, ‚ÄúA semantics aware random forest for text classification,‚Äù in Proc. CIKM,
2019, pp. 1061‚Äì1070, 2019.
[60] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, ‚ÄúSemi-supervised recursive autoencoders for
predicting sentiment distributions,‚Äù in Proc. EMNLP, 2011 , pp. 151‚Äì161, 2011.
[61] ‚ÄúA MATLAB implementation of RAE.‚Äù https://github.com/vin00/Semi-Supervised-Recursive-Autoencoders-for-
Predicting-Sentiment-Distributions, 2011.
[62] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, ‚ÄúSemantic compositionality through recursive matrix-vector spaces,‚Äù
inProc. EMNLP, 2012 , pp. 1201‚Äì1211, 2012.
[63] ‚ÄúA Tensorflow implementation of MV_RNN.‚Äù https://github.com/github-pengge/MV_RNN, 2012.
[64] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, ‚ÄúRecursive deep models for semantic
compositionality over a sentiment treebank,‚Äù in Proc. EMNLP, 2013 , pp. 1631‚Äì1642, 2013.
[65] ‚ÄúA MATLAB implementation of RNTN.‚Äù https://github.com/pondruska/DeepSentiment, 2013.
[66] O. Irsoy and C. Cardie, ‚ÄúDeep recursive neural networks for compositionality in language,‚Äù in Proc. NIPS, 2014 ,
pp. 2096‚Äì2104, 2014.
[67] Q. V. Le and T. Mikolov, ‚ÄúDistributed representations of sentences and documents, ‚Äù in Proc. ICML, 2014 , pp. 1188‚Äì1196,
2014.
[68] ‚ÄúA PyTorch implementation of Paragraph Vectors (doc2vec).‚Äù https://github.com/inejc/paragraph-vectors, 2014.
[69] M. Iyyer, V. Manjunatha, J. L. Boyd-Graber, and H. D. III, ‚ÄúDeep unordered composition rivals syntactic methods for
text classification,‚Äù in Proc. ACL, 2015 , pp. 1681‚Äì1691, 2015.
[70] ‚ÄúAn implementation of DAN.‚Äù https://github.com/miyyer/dan, 2015.
[71] ‚ÄúA PyTorch implementation of Tree-LSTM.‚Äù https://github.com/stanfordnlp/treelstm, 2015.
[72] S. Lai, L. Xu, K. Liu, and J. Zhao, ‚ÄúRecurrent convolutional neural networks for text classification,‚Äù AAAI‚Äô15,
p. 2267‚Äì2273, AAAI Press, 2015.
[73] ‚ÄúA Tensorflow implementation of TextRCNN.‚Äù https://github.com/roomylee/rcnn-text-classification, 2015.
[74] ‚ÄúAn implementation of MT-LSTM.‚Äù https://github.com/AlexAntn/MTLSTM, 2015.
[75] R. Johnson and T. Zhang, ‚ÄúSupervised and semi-supervised text categorization using LSTM for region embeddings,‚Äù
inProc. ICML, 2016 , pp. 526‚Äì534, 2016.
[76] ‚ÄúAn implementation of oh-2LSTMp.‚Äù http://riejohnson.com/cnn_20download.html, 2015.
[77] P. Zhou, Z. Qi, S. Zheng, J. Xu, H. Bao, and B. Xu, ‚ÄúText classification improved by integrating bidirectional LSTM
with two-dimensional max pooling,‚Äù in Proc. COLING, 2016 , pp. 3485‚Äì3495, 2016.
[78] ‚ÄúAn implementation of BLSTM-2DCNN.‚Äù https://github.com/ManuelVs/NNForTextClassification, 2016.
[79] P. Liu, X. Qiu, and X. Huang, ‚ÄúRecurrent neural network for text classification with multi-task learning,‚Äù in Proc.
IJCAI, 2016 , pp. 2873‚Äì2879, 2016.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:32 Qian Li, et al.
[80] ‚ÄúA PyTorch implementation of Multi-Task.‚Äù https://github.com/baixl/text_classification, 2016.
[81] B. Felbo, A. Mislove, A. S√∏gaard, I. Rahwan, and S. Lehmann, ‚ÄúUsing millions of emoji occurrences to learn any-domain
representations for detecting sentiment, emotion and sarcasm,‚Äù in Proc. EMNLP, 2017 , pp. 1615‚Äì1625, 2017.
[82] ‚ÄúA Keras implementation of DeepMoji.‚Äù https://github.com/bfelbo/DeepMoji, 2018.
[83] A. B. Dieng, C. Wang, J. Gao, and J. W. Paisley, ‚ÄúTopicrnn: A recurrent neural network with long-range semantic
dependency,‚Äù in Proc. ICLR, 2017 , 2017.
[84] ‚ÄúA PyTorch implementation of TopicRNN.‚Äù https://github.com/dangitstam/topic-rnn, 2017.
[85] T. Miyato, A. M. Dai, and I. J. Goodfellow, ‚ÄúAdversarial training methods for semi-supervised text classification,‚Äù in
Proc. ICLR, 2017 , 2017.
[86] ‚ÄúA Tensorflow implementation of Virtual adversarial training.‚Äù https://github.com/tensorflow/models/tree/master/
adversarial_text, 2017.
[87] Y. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, ‚ÄúSentiment analysis by capsules,‚Äù in Proc. WWW, 2018 , pp. 1165‚Äì1174,
2018.
[88] ‚ÄúA PyTorch implementation of RNN-Capsule.‚Äù https://github.com/wangjiosw/Sentiment-Analysis-by-Capsules, 2018.
[89] Y. Zhao, Y. Shen, and J. Yao, ‚ÄúRecurrent neural network for text classification with hierarchical multiscale dense
connections,‚Äù in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019,
Macao, China, August 10-16, 2019 , pp. 5450‚Äì5456, 2019.
[90] ‚ÄúAn implementation of HM-DenseRNNs.‚Äù https://github.com/zhaoyizhaoyi/hm-densernns, 2019.
[91] ‚ÄúA Keras implementation of TextCNN.‚Äù https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-
Keras, 2014.
[92] ‚ÄúA Tensorflow implementation of DCNN.‚Äù https://github.com/kinimod23/ATS_Project, 2014.
[93] X. Zhang, J. J. Zhao, and Y. LeCun, ‚ÄúCharacter-level convolutional networks for text classification,‚Äù in Proc. NeurIPS,
2015, pp. 649‚Äì657, 2015.
[94] ‚ÄúA Tensorflow implementation of CharCNN.‚Äù https://github.com/mhjabreel/CharCNN, 2015.
[95] ‚ÄúA Keras implementation of SeqTextRCNN.‚Äù https://github.com/ilimugur/short-text-classification, 2016.
[96] J. Liu, W. Chang, Y. Wu, and Y. Yang, ‚ÄúDeep learning for extreme multi-label text classification,‚Äù in Proc. ACM SIGIR,
2017, pp. 115‚Äì124, 2017.
[97] ‚ÄúA Pytorch implementation of XML-CNN.‚Äù https://github.com/siddsax/XML-CNN, 2017.
[98] R. Johnson and T. Zhang, ‚ÄúDeep pyramid convolutional neural networks for text categorization,‚Äù in Proc. ACL, 2017 ,
pp. 562‚Äì570, 2017.
[99] ‚ÄúA PyTorch implementation of DPCNN.‚Äù https://github.com/Cheneng/DPCNN, 2017.
[100] J. Wang, Z. Wang, D. Zhang, and J. Yan, ‚ÄúCombining knowledge with deep convolutional neural networks for short
text classification,‚Äù in Proc. IJCAI, 2017 , pp. 2915‚Äì2921, 2017.
[101] M. Yang, W. Zhao, J. Ye, Z. Lei, Z. Zhao, and S. Zhang, ‚ÄúInvestigating capsule networks with dynamic routing for text
classification,‚Äù in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels,
Belgium, October 31 - November 4, 2018 , pp. 3110‚Äì3119, 2018.
[102] ‚ÄúA Tensorflow implementation of TextCapsule.‚Äù https://github.com/andyweizhao/capsule_text_classification, 2018.
[103] K. Shimura, J. Li, and F. Fukumoto, ‚ÄúHFT-CNN: learning hierarchical category structure for multi-label short text
categorization,‚Äù in Proc. EMNLP, 2018 , pp. 811‚Äì816, 2018.
[104] ‚ÄúAn implementation of HFT-CNN.‚Äù https://github.com/ShimShim46/HFT-CNN, 2018.
[105] J. Xu and Y. Cai, ‚ÄúIncorporating context-relevant knowledge into convolutional neural networks for short text
classification,‚Äù in The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative
Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in
Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019 , pp. 10067‚Äì10068, 2019.
[106] Y. Bao, M. Wu, S. Chang, and R. Barzilay, ‚ÄúFew-shot text classification with distributional signatures,‚Äù in Proc. ICLR,
2020, 2020.
[107] ‚ÄúA PyTorch implementation of few-shot text classification with distributional signatures.‚Äù https://github.com/YujiaBao/
Distributional-Signatures, 2020.
[108] Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy, ‚ÄúHierarchical attention networks for document
classification,‚Äù in Proc. NAACL, 2016 , pp. 1480‚Äì1489, 2016.
[109] ‚ÄúA Keras implementation of TextCNN.‚Äù https://github.com/richliao/textClassifier, 2014.
[110] X. Zhou, X. Wan, and J. Xiao, ‚ÄúAttention-based LSTM network for cross-lingual sentiment classification,‚Äù in Proc.
EMNLP, 2016 , pp. 247‚Äì256, 2016.
[111] ‚ÄúNLP&CC Corpus.‚Äù http://tcci.ccf.org.cn/conference/2013/index.html, 2013.
[112] J. Cheng, L. Dong, and M. Lapata, ‚ÄúLong short-term memory-networks for machine reading,‚Äù in Proc. EMNLP, 2016 ,
pp. 551‚Äì561, 2016.
[113] ‚ÄúA Tensorflow implementation of LSTMN.‚Äù https://github.com/JRC1995/Abstractive-Summarization, 2016.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:33
[114] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, and Y. Bengio, ‚ÄúA structured self-attentive sentence
embedding,‚Äù in Proc. ICLR, 2017 , 2017.
[115] ‚ÄúA PyTorch implementation of Structured-Self-Attention.‚Äù https://github.com/kaushalshetty/Structured-Self-
Attention, 2017.
[116] P. Yang, X. Sun, W. Li, S. Ma, W. Wu, and H. Wang, ‚ÄúSGM: sequence generation model for multi-label classification,‚Äù
inProc. COLING, 2018 , pp. 3915‚Äì3926, 2018.
[117] ‚ÄúA PyTorch implementation of SGM.‚Äù https://github.com/lancopku/SGM, 2018.
[118] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, ‚ÄúDeep contextualized word
representations,‚Äù in Proc. NAACL, 2018 , pp. 2227‚Äì2237, 2018.
[119] ‚ÄúA PyTorch implementation of ELMo.‚Äù https://github.com/flairNLP/flair, 2018.
[120] T. Shen, T. Zhou, G. Long, J. Jiang, and C. Zhang, ‚ÄúBi-directional block self-attention for fast and memory-efficient
sequence modeling,‚Äù in Proc. ICLR, 2018 , 2018.
[121] ‚ÄúA PyTorch implementation of BiBloSA.‚Äù https://github.com/galsang/BiBloSA-pytorch, 2018.
[122] R. You, Z. Zhang, Z. Wang, S. Dai, H. Mamitsuka, and S. Zhu, ‚ÄúAttentionxml: Label tree-based attention-aware deep
model for high-performance extreme multi-label text classification,‚Äù in Proc. NeurIPS, 2019 , pp. 5812‚Äì5822, 2019.
[123] ‚ÄúA PyTorch implementation of AttentionXML.‚Äù https://github.com/yourh/AttentionXML, 2019.
[124] S. Sun, Q. Sun, K. Zhou, and T. Lv, ‚ÄúHierarchical attention prototypical networks for few-shot text classification,‚Äù in
Proc. EMNLP, 2019 , pp. 476‚Äì485, 2019.
[125] T. Gao, X. Han, Z. Liu, and M. Sun, ‚ÄúHybrid attention-based prototypical networks for noisy few-shot relation
classification,‚Äù in Proc. AAAI, 2019 , pp. 6407‚Äì6414, 2019.
[126] ‚ÄúA PyTorch implementation of HATT-Proto.‚Äù https://github.com/thunlp/HATT-Proto, 2019.
[127] J. Chen, Y. Hu, J. Liu, Y. Xiao, and H. Jiang, ‚ÄúDeep short text classification with knowledge powered attention,‚Äù in Proc.
AAAI, 2019 , pp. 6252‚Äì6259, 2019.
[128] ‚ÄúA PyTorch implementation of STCKA.‚Äù https://github.com/AIRobotZhang/STCKA, 2019.
[129] K. Ding, J. Wang, J. Li, D. Li, and H. Liu, ‚ÄúBe more with less: Hypergraph attention networks for inductive text
classification,‚Äù in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP
2020, Online, November 16-20, 2020 , pp. 4927‚Äì4936, 2020.
[130] ‚ÄúA pytorch implementation of HyperGAT.‚Äù https://github.com/kaize0409/HyperGAT, 2020.
[131] Q. Guo, X. Qiu, P. Liu, X. Xue, and Z. Zhang, ‚ÄúMulti-scale self-attention for text classification,‚Äù in The Thirty-Fourth
AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New
York, NY, USA, February 7-12, 2020 , pp. 7847‚Äì7854, 2020.
[132] S. Choi, H. Park, J. Yeo, and S. Hwang, ‚ÄúLess is more: Attention supervision with counterfactuals for text classification,‚Äù
inProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online,
November 16-20, 2020 , pp. 6695‚Äì6704, 2020.
[133] ‚ÄúA Tensorflow implementation of BERT.‚Äù https://github.com/google-research/bert, 2019.
[134] I. Chalkidis, M. Fergadiotis, P. Malakasiotis, and I. Androutsopoulos, ‚ÄúLarge-scale multi-label text classification on EU
legislation,‚Äù in Proc. ACL, 2019 , pp. 6314‚Äì6322, 2019.
[135] ‚ÄúA Tensorflow implementation of BERT-BASE.‚Äù https://github.com/iliaschalkidis/lmtc-eurlex57k, 2019.
[136] C. Sun, X. Qiu, Y. Xu, and X. Huang, ‚ÄúHow to fine-tune BERT for text classification?,‚Äù in Proc. CCL, 2019 , pp. 194‚Äì206,
2019.
[137] ‚ÄúA Tensorflow implementation of BERT4doc-Classification.‚Äù https://github.com/xuyige/BERT4doc-Classification,
2019.
[138] Z. Yang, Z. Dai, Y. Yang, J. G. Carbonell, R. Salakhutdinov, and Q. V. Le, ‚ÄúXlnet: Generalized autoregressive pretraining
for language understanding,‚Äù in Proc. NeurIPS, 2019 , pp. 5754‚Äì5764, 2019.
[139] ‚ÄúA Tensorflow implementation of XLNet.‚Äù https://github.com/zihangdai/xlnet, 2019.
[140] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, ‚ÄúRoberta: A
robustly optimized BERT pretraining approach,‚Äù CoRR , vol. abs/1907.11692, 2019.
[141] ‚ÄúA PyTorch implementation of RoBERTa.‚Äù https://github.com/pytorch/fairseq, 2019.
[142] D. Croce, G. Castellucci, and R. Basili, ‚ÄúGAN-BERT: generative adversarial learning for robust text classification
with a bunch of labeled examples,‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020 , pp. 2114‚Äì2119, 2020.
[143] ‚ÄúA pytorch implementation of GAN-BERT.‚Äù https://github.com/crux82/ganbert, 2020.
[144] S. Garg and G. Ramakrishnan, ‚ÄúBAE: bert-based adversarial examples for text classification,‚Äù in Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 ,
pp. 6174‚Äì6181, 2020.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:34 Qian Li, et al.
[145] ‚ÄúAn implementation of BAE.‚Äù https://github.com/QData/TextAttack/blob/master/textattack/attack_recipes/bae_garg_
2019.py, 2020.
[146] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, ‚ÄúALBERT: A lite BERT for self-supervised learning
of language representations,‚Äù in Proc. ICLR, 2020 , 2020.
[147] ‚ÄúA Tensorflow implementation of ALBERT.‚Äù https://github.com/google-research/ALBERT, 2020.
[148] H. Zhang and J. Zhang, ‚ÄúText graph transformer for document classification,‚Äù in Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020 , pp. 8322‚Äì8327, 2020.
[149] W. Chang, H. Yu, K. Zhong, Y. Yang, and I. S. Dhillon, ‚ÄúTaming pretrained transformers for extreme multi-label text
classification,‚Äù in KDD ‚Äô20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,
CA, USA, August 23-27, 2020 , pp. 3163‚Äì3171, 2020.
[150] ‚ÄúAn implementation of X-Transformer.‚Äù https://github.com/OctoberChang/X-Transformer, 2020.
[151] T. Jiang, D. Wang, L. Sun, H. Yang, Z. Zhao, and F. Zhuang, ‚ÄúLightxml: Transformer with dynamic negative sampling
for high-performance extreme multi-label text classification,‚Äù CoRR , vol. abs/2101.03305, 2021.
[152] ‚ÄúAn implementation of LightXML.‚Äù https://github.com/kongds/LightXML, 2021.
[153] H. Peng, J. Li, Y. He, Y. Liu, M. Bao, L. Wang, Y. Song, and Q. Yang, ‚ÄúLarge-scale hierarchical text classification with
recursively regularized deep graph-cnn,‚Äù in Proc. WWW, 2018 , pp. 1063‚Äì1072, 2018.
[154] ‚ÄúA Tensorflow implementation of DeepGraphCNNforTexts.‚Äù https://github.com/HKUST-KnowComp/
DeepGraphCNNforTexts, 2018.
[155] L. Yao, C. Mao, and Y. Luo, ‚ÄúGraph convolutional networks for text classification,‚Äù in Proc. AAAI, 2019 , pp. 7370‚Äì7377,
2019.
[156] ‚ÄúA Tensorflow implementation of TextGCN.‚Äù https://github.com/yao8839836/text_gcn, 2019.
[157] F. Wu, A. H. S. Jr., T. Zhang, C. Fifty, T. Yu, and K. Q. Weinberger, ‚ÄúSimplifying graph convolutional networks,‚Äù in
Proc. ICML, 2019 , pp. 6861‚Äì6871, 2019.
[158] ‚ÄúAn implementation of SGC.‚Äù https://github.com/Tiiiger/SGC, 2019.
[159] L. Huang, D. Ma, S. Li, X. Zhang, and H. Wang, ‚ÄúText level graph neural network for text classification,‚Äù in Proc.
EMNLP, 2019 , pp. 3442‚Äì3448, 2019.
[160] ‚ÄúAn implementation of TextLevelGNN.‚Äù https://github.com/LindgeW/TextLevelGNN, 2019.
[161] H. Peng, J. Li, S. Wang, L. Wang, Q. Gong, R. Yang, B. Li, P. Yu, and L. He, ‚ÄúHierarchical taxonomy-aware and
attentional graph capsule rcnns for large-scale multi-label text classification,‚Äù IEEE Transactions on Knowledge and
Data Engineering , 2019.
[162] Y. Zhang, X. Yu, Z. Cui, S. Wu, Z. Wen, and L. Wang, ‚ÄúEvery document owns its structure: Inductive text classification
via graph neural networks,‚Äù in Proc. ACL, 2020 , pp. 334‚Äì339, 2020.
[163] ‚ÄúA Tensorflow implementation of TextING.‚Äù https://github.com/CRIPAC-DIG/TextING, 2019.
[164] X. Liu, X. You, X. Zhang, J. Wu, and P. Lv, ‚ÄúTensor graph convolutional networks for text classification,‚Äù in The Thirty-
Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial
Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI
2020, New York, NY, USA, February 7-12, 2020 , pp. 8409‚Äì8416, 2020.
[165] ‚ÄúA Tensorflow implementation of TensorGCN.‚Äù https://github.com/THUMLP/TensorGCN, 2019.
[166] A. Pal, M. Selvakumar, and M. Sankarasubbu, ‚ÄúMAGNET: multi-label text classification using attention-based graph
neural network,‚Äù in Proc. ICAART, 2020 , pp. 494‚Äì505, 2020.
[167] ‚ÄúA repository of MAGNET.‚Äù https://github.com/monk1337/MAGnet, 2020.
[168] ‚ÄúA Tensorflow implementation of Miyato et al..‚Äù https://github.com/TobiasLee/Text-Classification, 2017.
[169] J. Zeng, J. Li, Y. Song, C. Gao, M. R. Lyu, and I. King, ‚ÄúTopic memory networks for short text classification,‚Äù in Proc.
EMNLP, 2018 , pp. 3120‚Äì3131, 2018.
[170] J. Zhang, P. Lertvittayakumjorn, and Y. Guo, ‚ÄúIntegrating semantic knowledge to tackle zero-shot text classification,‚Äù
inProc. NAACL, 2019 , pp. 1031‚Äì1040, 2019.
[171] ‚ÄúA Tensorflow implementation of KG4ZeroShotText.‚Äù https://github.com/JingqingZ/KG4ZeroShotText, 2019.
[172] M. k. Alsmadi, K. B. Omar, S. A. Noah, and I. Almarashdah, ‚ÄúPerformance comparison of multi-layer perceptron
(back propagation, delta rule and perceptron) algorithms in neural networks,‚Äù in 2009 IEEE International Advance
Computing Conference , pp. 296‚Äì299, 2009.
[173] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. E. P. Reyes, M. Shyu, S. Chen, and S. S. Iyengar, ‚ÄúA survey on deep
learning: Algorithms, techniques, and applications,‚Äù ACM Comput. Surv. , vol. 51, no. 5, pp. 92:1‚Äì92:36, 2019.
[174] L. Qin, W. Che, Y. Li, M. Ni, and T. Liu, ‚ÄúDcr-net: A deep co-interactive relation network for joint dialog act recognition
and sentiment classification,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-
Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 8665‚Äì8672, 2020.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:35
[175] Z. Deng, H. Peng, D. He, J. Li, and P. S. Yu, ‚ÄúHtcinfomax: A global model for hierarchical text classification via
information maximization,‚Äù in Proceedings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021 (K. Toutanova,
A. Rumshisky, L. Zettlemoyer, D. Hakkani-T√ºr, I. Beltagy, S. Bethard, R. Cotterell, T. Chakraborty, and Y. Zhou, eds.),
pp. 3259‚Äì3265, Association for Computational Linguistics, 2021.
[176] D. Jin, Z. Jin, J. T. Zhou, and P. Szolovits, ‚ÄúIs BERT really robust? A strong baseline for natural language attack on
text classification and entailment,‚Äù in The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on
Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp. 8018‚Äì8025, 2020.
[177] C. Li, X. Peng, H. Peng, J. Li, and L. Wang, ‚ÄúTextgtl: Graph-based transductive learning for semi-supervised textclassi-
fication via structure-sensitive interpolation,‚Äù in IJCAI 2021 , ijcai.org, 2021.
[178] T. Miyato, S. Maeda, M. Koyama, and S. Ishii, ‚ÄúVirtual adversarial training: a regularization method for supervised
and semi-supervised learning,‚Äù CoRR , vol. abs/1704.03976, 2017.
[179] G. E. Hinton, A. Krizhevsky, and S. D. Wang, ‚ÄúTransforming auto-encoders,‚Äù in Proc. ICANN, 2011 (T. Honkela, W. Duch,
M. Girolami, and S. Kaski, eds.), (Berlin, Heidelberg), pp. 44‚Äì51, Springer Berlin Heidelberg, 2011.
[180] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural Computation , vol. 9, no. 8, pp. 1735‚Äì1780, 1997.
[181] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, ‚ÄúA large annotated corpus for learning natural language
inference,‚Äù in Proc. EMNLP, 2015 , pp. 632‚Äì642, 2015.
[182] Z. Wang, W. Hamza, and R. Florian, ‚ÄúBilateral multi-perspective matching for natural language sentences,‚Äù in Proc.
IJCAI, 2017 , pp. 4144‚Äì4150, 2017.
[183] R. Johnson and T. Zhang, ‚ÄúSemi-supervised convolutional neural networks for text categorization via region embed-
ding,‚Äù in Proc. NeurIPS, 2015 , pp. 919‚Äì927, 2015.
[184] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúIdentity mappings in deep residual networks,‚Äù in Proc. ECCV, 2016 , pp. 630‚Äì645,
2016.
[185] I. Bazzi, Modelling out-of-vocabulary words for robust speech recognition . PhD thesis, Massachusetts Institute of
Technology, 2002.
[186] H. Nguyen and M. Nguyen, ‚ÄúA deep neural architecture for sentence-level sentiment classification in twitter social
networking,‚Äù in Proc. PACLING, 2017 , pp. 15‚Äì27, 2017.
[187] B. Adams and G. McKenzie, ‚ÄúCrowdsourcing the character of a place: Character-level convolutional networks for
multilingual geographic text classification,‚Äù Trans. GIS , vol. 22, no. 2, pp. 394‚Äì408, 2018.
[188] Z. Chen and T. Qian, ‚ÄúTransfer capsule network for aspect level sentiment classification, ‚Äù in Proc. ACL, 2019 , pp. 547‚Äì556,
2019.
[189] W. Xue, W. Zhou, T. Li, and Q. Wang, ‚ÄúMTNA: A neural multi-task model for aspect category classification and aspect
term extraction on restaurant reviews,‚Äù in Proc. IJCNLP, 2017 , pp. 151‚Äì156, 2017.
[190] D. Bahdanau, K. Cho, and Y. Bengio, ‚ÄúNeural machine translation by jointly learning to align and translate,‚Äù in Proc.
ICLR, 2015 , 2015.
[191] Z. Hu, X. Li, C. Tu, Z. Liu, and M. Sun, ‚ÄúFew-shot charge prediction with discriminative legal attributes,‚Äù in Proc.
COLING, 2018 , pp. 487‚Äì498, 2018.
[192] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all
you need,‚Äù in Proc. NeurIPS, 2017 , pp. 5998‚Äì6008, 2017.
[193] Y. Ma, H. Peng, and E. Cambria, ‚ÄúTargeted aspect-based sentiment analysis via embedding commonsense knowledge
into an attentive LSTM,‚Äù in Proc. AAAI, 2018 , pp. 5876‚Äì5883, 2018.
[194] Y. Wang, M. Huang, X. Zhu, and L. Zhao, ‚ÄúAttention-based LSTM for aspect-level sentiment classification,‚Äù in Proc.
EMNLP, 2016 , pp. 606‚Äì615, 2016.
[195] F. Fan, Y. Feng, and D. Zhao, ‚ÄúMulti-grained attention network for aspect-level sentiment classification,‚Äù in Proc.
EMNLP, 2018 , pp. 3433‚Äì3442, 2018.
[196] M. Tan, C. dos Santos, B. Xiang, and B. Zhou, ‚ÄúImproved representation learning for question answer matching,‚Äù in
Proc. ACL, 2016 , (Berlin, Germany), pp. 464‚Äì473, Association for Computational Linguistics, Aug. 2016.
[197] C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou, ‚ÄúAttentive pooling networks,‚Äù CoRR , vol. abs/1602.03609, 2016.
[198] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, ‚ÄúPre-trained models for natural language processing: A survey,‚Äù
CoRR , vol. abs/2003.08271, 2020.
[199] A. Radford, ‚ÄúImproving language understanding by generative pre-training,‚Äù 2018.
[200] Z. Dai, Z. Yang, Y. Yang, J. G. Carbonell, Q. V. Le, and R. Salakhutdinov, ‚ÄúTransformer-xl: Attentive language models
beyond a fixed-length context,‚Äù in Proc. ACL, 2019 , pp. 2978‚Äì2988, 2019.
[201] G. Jawahar, B. Sagot, and D. Seddah, ‚ÄúWhat does BERT learn about the structure of language?,‚Äù in Proc. ACL, 2019 ,
pp. 3651‚Äì3657, 2019.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:36 Qian Li, et al.
[202] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, ‚ÄúBART:
denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,‚Äù in
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10,
2020, pp. 7871‚Äì7880, 2020.
[203] M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, ‚ÄúSpanbert: Improving pre-training by representing
and predicting spans,‚Äù Trans. Assoc. Comput. Linguistics , vol. 8, pp. 64‚Äì77, 2020.
[204] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian, D. Zhu, H. Tian, and H. Wu, ‚ÄúERNIE: enhanced representation
through knowledge integration,‚Äù CoRR , vol. abs/1904.09223, 2019.
[205] M. Defferrard, X. Bresson, and P. Vandergheynst, ‚ÄúConvolutional neural networks on graphs with fast localized
spectral filtering,‚Äù in Proc. NeurIPS, 2016 , pp. 3837‚Äì3845, 2016.
[206] H. Peng, R. Zhang, Y. Dou, R. Yang, J. Zhang, and P. S. Yu, ‚ÄúReinforced neighborhood selection guided multi-relational
graph neural networks,‚Äù arXiv preprint arXiv:2104.07886 , 2021.
[207] H. Peng, R. Yang, Z. Wang, J. Li, L. He, P. Yu, A. Zomaya, and R. Ranjan, ‚ÄúLime: Low-cost incremental learning for
dynamic heterogeneous information networks,‚Äù IEEE Transactions on Computers , 2021.
[208] J. Li, H. Peng, Y. Cao, Y. Dou, H. Zhang, P. Yu, and L. He, ‚ÄúHigher-order attribute-enhancing heterogeneous graph
neural networks,‚Äù IEEE Transactions on Knowledge and Data Engineering , 2021.
[209] D. Marcheggiani and I. Titov, ‚ÄúEncoding sentences with graph convolutional networks for semantic role labeling,‚Äù in
Proc. EMNLP, 2017 , pp. 1506‚Äì1515, 2017.
[210] Y. Li, R. Jin, and Y. Luo, ‚ÄúClassifying relations in clinical narratives using segment graph convolutional and recurrent
neural networks (seg-gcrns),‚Äù JAMIA , vol. 26, no. 3, pp. 262‚Äì268, 2019.
[211] J. Bastings, I. Titov, W. Aziz, D. Marcheggiani, and K. Sima‚Äôan, ‚ÄúGraph convolutional encoders for syntax-aware
neural machine translation,‚Äù in Proc. EMNLP, 2017 , pp. 1957‚Äì1967, 2017.
[212] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li√≤, and Y. Bengio, ‚ÄúGraph attention networks,‚Äù in Proc. ICLR,
2018, 2018.
[213] L. Hu, T. Yang, C. Shi, H. Ji, and X. Li, ‚ÄúHeterogeneous graph attention networks for semi-supervised short text
classification,‚Äù in Proc. EMNLP, 2019 , pp. 4820‚Äì4829, 2019.
[214] Z. Li, X. Ding, and T. Liu, ‚ÄúConstructing narrative event evolutionary graph for script event prediction,‚Äù in Proc. IJCAI,
2018, pp. 4201‚Äì4207, 2018.
[215] J. Bromley, I. Guyon, Y. LeCun, E. S√§ckinger, and R. Shah, ‚ÄúSignature verification using a siamese time delay neural
network,‚Äù in Proc. NeurIPS, 1993] , pp. 737‚Äì744, 1993.
[216] J. Mueller and A. Thyagarajan, ‚ÄúSiamese recurrent architectures for learning sentence similarity,‚Äù in Proc. AAAI, 2016 ,
pp. 2786‚Äì2792, 2016.
[217] Jayadeva, H. Pant, M. Sharma, and S. Soman, ‚ÄúTwin neural networks for the classification of large unbalanced datasets,‚Äù
Neurocomputing , vol. 343, pp. 34 ‚Äì 49, 2019. Learning in the Presence of Class Imbalance and Concept Drift.
[218] T. Miyato, S. ichi Maeda, M. Koyama, K. Nakae, and S. Ishii, ‚ÄúDistributional smoothing with virtual adversarial
training,‚Äù 2015.
[219] T. Zhang, M. Huang, and L. Zhao, ‚ÄúLearning structured representation for text classification via reinforcement
learning,‚Äù in Proc. AAAI, 2018 , pp. 6053‚Äì6060, 2018.
[220] J. Weston, S. Chopra, and A. Bordes, ‚ÄúMemory networks,‚Äù 2015.
[221] X. Li and W. Lam, ‚ÄúDeep multi-task learning for aspect term extraction with memory interaction,‚Äù in Proc. EMNLP,
2017, pp. 2886‚Äì2892, 2017.
[222] C. Shen, C. Sun, J. Wang, Y. Kang, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, ‚ÄúSentiment classification towards
question-answering with hierarchical matching network,‚Äù in Proc. EMNLP, 2018 , pp. 3654‚Äì3663, 2018.
[223] X. Ding, K. Liao, T. Liu, Z. Li, and J. Duan, ‚ÄúEvent representation learning enhanced with external commonsense
knowledge,‚Äù in Proc. EMNLP, 2019 , pp. 4893‚Äì4902, 2019.
[224] Y. Zhang, D. Song, P. Zhang, X. Li, and P. Wang, ‚ÄúA quantum-inspired sentiment representation model for twitter
sentiment analysis,‚Äù Applied Intelligence , 2019.
[225] O. Zafrir, G. Boudoukh, P. Izsak, and M. Wasserblat, ‚ÄúQ8BERT: quantized 8bit BERT,‚Äù in Fifth Workshop on Energy
Efficient Machine Learning and Cognitive Computing - NeurIPS Edition, EMC2@NeurIPS 2019, Vancouver, Canada,
December 13, 2019 , pp. 36‚Äì39, 2019.
[226] ‚ÄúMR Corpus.‚Äù http://www.cs.cornell.edu/people/pabo/movie-review-data/, 2002.
[227] ‚ÄúSST Corpus.‚Äù http://nlp.stanford.edu/sentiment, 2013.
[228] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts, ‚ÄúRecursive deep models for semantic
compositionality over a sentiment treebank,‚Äù in Proc. EMNLP, 2013 , (Seattle, Washington, USA), pp. 1631‚Äì1642,
Association for Computational Linguistics, Oct. 2013.
[229] ‚ÄúMPQA Corpus.‚Äù http://www.cs.pitt.edu/mpqa/, 2005.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:37
[230] Q. Diao, M. Qiu, C. Wu, A. J. Smola, J. Jiang, and C. Wang, ‚ÄúJointly modeling aspects, ratings and sentiments for
movie recommendation (JMARS),‚Äù in Proc. ACM SIGKDD, 2014 , pp. 193‚Äì202, 2014.
[231] D. Tang, B. Qin, and T. Liu, ‚ÄúDocument modeling with gated recurrent neural network for sentiment classification,‚Äù
inProc. EMNLP, 2015 , pp. 1422‚Äì1432, 2015.
[232] ‚ÄúAmazon review Corpus.‚Äù https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products, 2015.
[233] ‚ÄúTwitter Corpus.‚Äù https://www.cs.york.ac.uk/semeval-2013/task2/, 2013.
[234] ‚ÄúAG Corpus.‚Äù http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html, 2004.
[235] ‚ÄúReuters Corpus.‚Äù https://www.cs.umb.edu/~smimarog/textmining/datasets/, 2007.
[236] C. Wang, M. Zhang, S. Ma, and L. Ru, ‚ÄúAutomatic online news issue construction in web environment,‚Äù in Proc. WWW,
2008, pp. 457‚Äì466, 2008.
[237] X. Li, C. Li, J. Chi, J. Ouyang, and C. Li, ‚ÄúDataless text classification: A topic modeling approach with document
manifold,‚Äù in Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM
2018, Torino, Italy, October 22-26, 2018 , pp. 973‚Äì982, 2018.
[238] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef,
S. Auer, and C. Bizer, ‚ÄúDbpedia - A large-scale, multilingual knowledge base extracted from wikipedia,‚Äù Semantic Web ,
vol. 6, no. 2, pp. 167‚Äì195, 2015.
[239] ‚ÄúOhsumed Corpus.‚Äù http://davis.wpi.edu/xmdv/datasets/ohsumed.html, 2015.
[240] ‚ÄúEUR-Lex Corpus.‚Äù http://www.ke.tu-darmstadt.de/resources/eurlex/eurlex.html, 2019.
[241] ‚ÄúAmazon670K Corpus.‚Äù http://manikvarma.org/downloads/XC/XMLRepository.html, 2016.
[242] J. Yin and J. Wang, ‚ÄúA dirichlet multinomial mixture model-based approach for short text clustering, ‚Äù in The 20th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ‚Äô14, New York, NY, USA - August 24 -
27, 2014 , pp. 233‚Äì242, 2014.
[243] J. Chen, Z. Gong, W. Wang, W. Liu, M. Yang, and C. Wang, ‚ÄúTam: Targeted analysis model with reinforcement learning
on short texts,‚Äù IEEE Transactions on Neural Networks and Learning Systems , pp. 1‚Äì10, 2020.
[244] F. Wang, Z. Wang, Z. Li, and J. Wen, ‚ÄúConcept-based short text classification and ranking,‚Äù in Proc. CIKM, 2014 ,
pp. 1069‚Äì1078, 2014.
[245] ‚ÄúFudan Corpus.‚Äù www.datatang.com/data/44139and43543, 2015.
[246] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSquad: 100, 000+ questions for machine comprehension of text,‚Äù in
Proc. EMNLP, 2016 , pp. 2383‚Äì2392, 2016.
[247] X. Yao, B. V. Durme, C. Callison-Burch, and P. Clark, ‚ÄúAnswer extraction as sequence tagging with tree edit distance,‚Äù
inProc. NAACL, 2013 , pp. 858‚Äì867, 2013.
[248] ‚ÄúTREC Corpus.‚Äù https://cogcomp.seas.upenn.edu/Data/QA/QC/, 2002.
[249] Y. Yang, W. Yih, and C. Meek, ‚ÄúWikiqa: A challenge dataset for open-domain question answering,‚Äù in Proc. EMNLP,
2015, pp. 2013‚Äì2018, 2015.
[250] B. Pang and L. Lee, ‚ÄúA sentimental education: Sentiment analysis using subjectivity summarization based on minimum
cuts,‚Äù in Proc. ACL, 2004 , (Barcelona, Spain), pp. 271‚Äì278, July 2004.
[251] M. Hu and B. Liu, ‚ÄúMining and summarizing customer reviews,‚Äù in Proc. ACM SIGKDD, 2004 , pp. 168‚Äì177, 2004.
[252] ‚ÄúReuters Corpus.‚Äù https://martin-thoma.com/nlp-reuters, 2017.
[253] J. Kim, S. Jang, E. L. Park, and S. Choi, ‚ÄúText classification using capsules,‚Äù Neurocomputing , vol. 376, pp. 214‚Äì221,
2020.
[254] ‚ÄúReuters10 Corpus.‚Äù http://www.nltk.org/book/ch02.html, 2020.
[255] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li, ‚ÄúRCV1: A new benchmark collection for text categorization research,‚Äù J.
Mach. Learn. Res. , vol. 5, pp. 361‚Äì397, 2004.
[256] ‚ÄúRCV1-V2 Corpus.‚Äù http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/lyrl2004_rcv1v2_README.htm,
2004.
[257] B. Pang and L. Lee, ‚ÄúSeeing stars: Exploiting class relationships for sentiment categorization with respect to rating
scales,‚Äù in Proc. ACL, 2005 , pp. 115‚Äì124, 2005.
[258] J. Wiebe, T. Wilson, and C. Cardie, ‚ÄúAnnotating expressions of opinions and emotions in language, ‚Äù Language Resources
and Evaluation , vol. 39, no. 2-3, pp. 165‚Äì210, 2005.
[259] M. Thelwall, K. Buckley, and G. Paltoglou, ‚ÄúSentiment strength detection for the social web,‚Äù J. Assoc. Inf. Sci. Technol. ,
vol. 63, no. 1, pp. 163‚Äì173, 2012.
[260] P. Nakov, A. Ritter, S. Rosenthal, F. Sebastiani, and V. Stoyanov, ‚ÄúSemeval-2016 task 4: Sentiment analysis in twitter,‚Äù
inProc. SemEval, 2016) , 2016.
[261] Z. Lu, ‚ÄúPubmed and beyond: a survey of web tools for searching biomedical literature,‚Äù Database , vol. 2011, 2011.
[262] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, ‚ÄúMS MARCO: A human generated
machine reading comprehension dataset,‚Äù in Proc. NeurIPS, 2016 , 2016.
[263] https://data.quora.com/First-Quora-Dataset-Release-QuestionPairs.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.111:38 Qian Li, et al.
[264] P. Rajpurkar, R. Jia, and P. Liang, ‚ÄúKnow what you don‚Äôt know: Unanswerable questions for squad,‚Äù in Proc. ACL, 2018 ,
pp. 784‚Äì789, 2018.
[265] A. Williams, N. Nangia, and S. R. Bowman, ‚ÄúA broad-coverage challenge corpus for sentence understanding through
inference,‚Äù in Proc. NAACL, 2018 , pp. 1112‚Äì1122, 2018.
[266] M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and R. Zamparelli, ‚ÄúSemeval-2014 task 1: Evaluation of
compositional distributional semantic models on full sentences through semantic relatedness and textual entailment,‚Äù
inProc. SemEval, 2014 , pp. 1‚Äì8, 2014.
[267] B. Dolan, C. Quirk, and C. Brockett, ‚ÄúUnsupervised construction of large paraphrase corpora: Exploiting massively
parallel news sources,‚Äù in Proc. COLING, 2004 , 2004.
[268] D. M. Cer, M. T. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, ‚ÄúSemeval-2017 task 1: Semantic textual similarity -
multilingual and cross-lingual focused evaluation,‚Äù CoRR , vol. abs/1708.00055, 2017.
[269] I. Dagan, O. Glickman, and B. Magnini, ‚ÄúThe PASCAL recognising textual entailment challenge,‚Äù in Machine Learning
Challenges, Evaluating Predictive Uncertainty, Visual Object Classification and Recognizing Textual Entailment, First
PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected
Papers , pp. 177‚Äì190, 2005.
[270] T. Khot, A. Sabharwal, and P. Clark, ‚ÄúScitail: A textual entailment dataset from science question answering,‚Äù in
Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications
of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence
(EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018 , pp. 5189‚Äì5197, 2018.
[271] K. Kowsari, D. E. Brown, M. Heidarysafa, K. J. Meimandi, M. S. Gerber, and L. E. Barnes, ‚ÄúHdltex: Hierarchical deep
learning for text classification,‚Äù in Proc. ICMLA, 2017 , pp. 364‚Äì371, 2017.
[272] ‚ÄúAmazonCat-13K Corpus.‚Äù https://drive.google.com/open?id=1VwHAbri6y6oh8lkpZ6sSY_b1FRNnCLFL, 2018.
[273] ‚ÄúBlurbGenreCollection-EN Corpus.‚Äù https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/blurb-genre-
collection.html, 2017.
[274] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. √ì. S√©aghdha, S. Pad√≥, M. Pennacchiotti, L. Romano, and S. Szpakowicz,
‚ÄúSemeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals, ‚Äù in Proc. NAACL, 2009 ,
pp. 94‚Äì99, 2009.
[275] S. M. Strassel, M. A. Przybocki, K. Peterson, Z. Song, and K. Maeda, ‚ÄúLinguistic resources and evaluation techniques
for evaluation of cross-document automatic content extraction,‚Äù in Proc. LREC, 2008 , 2008.
[276] Y. Zhang, V. Zhong, D. Chen, G. Angeli, and C. D. Manning, ‚ÄúPosition-aware attention and supervised data improve
slot filling,‚Äù in Proc. EMNLP, 2017 , pp. 35‚Äì45, 2017.
[277] S. Riedel, L. Yao, and A. McCallum, ‚ÄúModeling relations and their mentions without labeled text,‚Äù in Proc. ECML PKDD,
2010, pp. 148‚Äì163, 2010.
[278] ‚ÄúFewRel Corpus.‚Äù https://github.com/thunlp/FewRel, 2019.
[279] S. Kim, L. F. D‚ÄôHaro, R. E. Banchs, J. D. Williams, and M. Henderson, ‚ÄúThe fourth dialog state tracking challenge,‚Äù in
Proc. IWSDS, 2016 , pp. 435‚Äì449, 2016.
[280] J. Ang, Y. Liu, and E. Shriberg, ‚ÄúAutomatic dialog act segmentation and classification in multiparty meetings,‚Äù in Proc.
ICASSP, 2005 , pp. 1061‚Äì1064, 2005.
[281] D. Jurafsky and E. Shriberg, ‚ÄúSwitchboard swbd-damsl shallow-discourse-function annotation coders manual,‚Äù 01
1997.
[282] A. Severyn and A. Moschitti, ‚ÄúLearning to rank short text pairs with convolutional deep neural networks,‚Äù in
Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,
Santiago, Chile, August 9-13, 2015 (R. Baeza-Yates, M. Lalmas, A. Moffat, and B. A. Ribeiro-Neto, eds.), pp. 373‚Äì382,
ACM, 2015.
[283] C. D. Manning, P. Raghavan, and H. Sch√ºtze, Introduction to information retrieval . Cambridge University Press, 2008.
[284] T. Nakagawa, K. Inui, and S. Kurohashi, ‚ÄúDependency tree-based sentiment classification using crfs with hidden vari-
ables,‚Äù in Human Language Technologies: Conference of the North American Chapter of the Association of Computational
Linguistics, Proceedings, June 2-4, 2010, Los Angeles, California, USA , pp. 786‚Äì794, 2010.
[285] J. Howard and S. Ruder, ‚ÄúUniversal language model fine-tuning for text classification,‚Äù in Proc. ACL, 2018 , pp. 328‚Äì339,
2018.
[286] G. Wang, C. Li, W. Wang, Y. Zhang, D. Shen, X. Zhang, R. Henao, and L. Carin, ‚ÄúJoint embedding of words and labels
for text classification,‚Äù in Proc. ACL, 2018 , pp. 2321‚Äì2331, 2018.
[287] X. Liu, P. He, W. Chen, and J. Gao, ‚ÄúMulti-task deep neural networks for natural language understanding,‚Äù in Proc.
ACL, 2019 , pp. 4487‚Äì4496, 2019.
[288] P. K. Pushp and M. M. Srivastava, ‚ÄúTrain once, test anywhere: Zero-shot learning for text classification,‚Äù CoRR ,
vol. abs/1712.05972, 2017.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.A Survey on Text Classification: From Traditional to Deep Learning 111:39
[289] C. Song, S. Zhang, N. Sadoughi, P. Xie, and E. P. Xing, ‚ÄúGeneralized zero-shot text classification for ICD coding,‚Äù in
Proc. IJCAI, 2020 , pp. 4018‚Äì4024, 2020.
[290] R. Geng, B. Li, Y. Li, X. Zhu, P. Jian, and J. Sun, ‚ÄúInduction networks for few-shot text classification,‚Äù in Proc. EMNLP,
2019, pp. 3902‚Äì3911, 2019.
[291] S. Deng, N. Zhang, Z. Sun, J. Chen, and H. Chen, ‚ÄúWhen low resource NLP meets unsupervised language model: Meta-
pretraining then meta-learning for few-shot text classification (student abstract),‚Äù in Proc. AAAI, 2020 , pp. 13773‚Äì13774,
2020.
[292] R. Geng, B. Li, Y. Li, J. Sun, and X. Zhu, ‚ÄúDynamic memory induction networks for few-shot text classification,‚Äù in
Proc. ACL, 2020 , pp. 1087‚Äì1094, 2020.
[293] K. R. Rojas, G. Bustamante, A. Oncevay, and M. A. S. Cabezudo, ‚ÄúEfficient strategies for hierarchical text classification:
External knowledge and auxiliary tasks, ‚Äù in Proceedings of the 58th Annual Meeting of the Association for Computational
Linguistics, ACL 2020, Online, July 5-10, 2020 , pp. 2252‚Äì2257, 2020.
[294] N. Shanavas, H. Wang, Z. Lin, and G. I. Hawe, ‚ÄúKnowledge-driven graph similarity for text classification,‚Äù Int. J. Mach.
Learn. Cybern. , vol. 12, no. 4, pp. 1067‚Äì1081, 2021.
[295] Y. Hao, Y. Zhang, K. Liu, S. He, Z. Liu, H. Wu, and J. Zhao, ‚ÄúAn end-to-end model for question answering over
knowledge base with cross-attention combining global knowledge,‚Äù in Proc. ACL, 2017 , pp. 221‚Äì231, 2017.
[296] R. T√ºrker, L. Zhang, M. Koutraki, and H. Sack, ‚ÄúTECNE: knowledge based text classification using network embeddings,‚Äù
inProc. EKAW, 2018 , pp. 53‚Äì56, 2018.
[297] X. Liang, D. Cheng, F. Yang, Y. Luo, W. Qian, and A. Zhou, ‚ÄúF-HMTC: detecting financial events for investment
decisions based on neural hierarchical multi-label text classification, ‚Äù in Proceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence, IJCAI 2020 , pp. 4490‚Äì4496, 2020.
[298] S. P. B., S. Modi, K. S. Hareesha, and S. Kumar, ‚ÄúClassification and comparison of malignancy detection of cervical
cells based on nucleus and textural features in microscopic images of uterine cervix,‚Äù Int. J. Medical Eng. Informatics ,
vol. 13, no. 1, pp. 1‚Äì13, 2021.
[299] T. Wang, L. Liu, N. Liu, H. Zhang, L. Zhang, and S. Feng, ‚ÄúA multi-label text classification method via dynamic
semantic representation model and deep neural network,‚Äù Appl. Intell. , vol. 50, no. 8, pp. 2339‚Äì2351, 2020.
[300] B. Wang, X. Hu, P. Li, and P. S. Yu, ‚ÄúCognitive structure learning model for hierarchical multi-label text classification,‚Äù
Knowl. Based Syst. , vol. 218, p. 106876, 2021.
[301] J. Du, Y. Huang, and K. Moilanen, ‚ÄúPointing to select: A fast pointer-lstm for long text classification, ‚Äù in Proceedings of
the 28th International Conference on Computational Linguistics, COLING 2020, Barcelona, Spain (Online), December
8-13, 2020 , pp. 6184‚Äì6193, 2020.
[302] J. Du, C. Vong, and C. L. P. Chen, ‚ÄúNovel efficient RNN and lstm-like architectures: Recurrent and gated broad learning
systems and their applications for text classification,‚Äù IEEE Trans. Cybern. , vol. 51, no. 3, pp. 1586‚Äì1597, 2021.
[303] T. Kanchinadam, Q. You, K. Westpfahl, J. Kim, S. Gunda, S. Seith, and G. Fung, ‚ÄúA simple yet brisk and efficient active
learning platform for text classification,‚Äù CoRR , vol. abs/2102.00426, 2021.
[304] Y. Zhou, J. Jiang, K. Chang, and W. Wang, ‚ÄúLearning to discriminate perturbations for blocking adversarial attacks in
text classification,‚Äù in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November
3-7, 2019 , pp. 4903‚Äì4912, 2019.
[305] A. Azizi, I. A. Tahmid, A. Waheed, N. Mangaokar, J. Pu, M. Javed, C. K. Reddy, and B. Viswanath, ‚ÄúT-miner: A
generative approach to defend against trojan attacks on dnn-based text classification,‚Äù CoRR , vol. abs/2103.04264,
2021.
ACM Trans. Intell. Syst. Technol., Vol. 37, No. 4, Article 111. Publication date: April 2021.